{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import AdamW, lr_scheduler\n",
    "import transforms as T\n",
    "from torchvision import datasets, ops\n",
    "from torchvision.models.feature_extraction import create_feature_extractor\n",
    "from einops import rearrange\n",
    "from pycocotools import mask as coco_mask\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.optimize import linear_sum_assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_coco_poly_to_mask(segmentations, height, width):\n",
    "    masks = []\n",
    "    for polygons in segmentations:\n",
    "        rles = coco_mask.frPyObjects(polygons, height, width)\n",
    "        mask = coco_mask.decode(rles)\n",
    "        if len(mask.shape) < 3:\n",
    "            mask = mask[..., None]\n",
    "        mask = torch.as_tensor(mask, dtype=torch.uint8)\n",
    "        mask = mask.any(dim=2)\n",
    "        masks.append(mask)\n",
    "    if masks:\n",
    "        masks = torch.stack(masks, dim=0)\n",
    "    else:\n",
    "        masks = torch.zeros((0, height, width), dtype=torch.uint8)\n",
    "    return masks\n",
    "\n",
    "class ConvertCocoPolysToMask(object):\n",
    "    def __init__(self, return_masks=False):\n",
    "        self.return_masks = return_masks\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        w, h = image.size\n",
    "\n",
    "        image_id = target[\"image_id\"]\n",
    "        image_id = torch.tensor([image_id])\n",
    "\n",
    "        anno = target[\"annotations\"]\n",
    "\n",
    "        anno = [obj for obj in anno if 'iscrowd' not in obj or obj['iscrowd'] == 0]\n",
    "\n",
    "        boxes = [obj[\"bbox\"] for obj in anno]\n",
    "        # guard against no boxes via resizing\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32).reshape(-1, 4)\n",
    "        boxes[:, 2:] += boxes[:, :2]\n",
    "        boxes[:, 0::2].clamp_(min=0, max=w)\n",
    "        boxes[:, 1::2].clamp_(min=0, max=h)\n",
    "\n",
    "        classes = [obj[\"category_id\"] for obj in anno]\n",
    "        classes = torch.tensor(classes, dtype=torch.int64)\n",
    "\n",
    "        if self.return_masks:\n",
    "            segmentations = [obj[\"segmentation\"] for obj in anno]\n",
    "            masks = convert_coco_poly_to_mask(segmentations, h, w)\n",
    "\n",
    "        keypoints = None\n",
    "        if anno and \"keypoints\" in anno[0]:\n",
    "            keypoints = [obj[\"keypoints\"] for obj in anno]\n",
    "            keypoints = torch.as_tensor(keypoints, dtype=torch.float32)\n",
    "            num_keypoints = keypoints.shape[0]\n",
    "            if num_keypoints:\n",
    "                keypoints = keypoints.view(num_keypoints, -1, 3)\n",
    "\n",
    "        keep = (boxes[:, 3] > boxes[:, 1]) & (boxes[:, 2] > boxes[:, 0])\n",
    "        boxes = boxes[keep]\n",
    "        classes = classes[keep]\n",
    "        if self.return_masks:\n",
    "            masks = masks[keep]\n",
    "        if keypoints is not None:\n",
    "            keypoints = keypoints[keep]\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = classes\n",
    "        if self.return_masks:\n",
    "            target[\"masks\"] = masks\n",
    "        target[\"image_id\"] = image_id\n",
    "        if keypoints is not None:\n",
    "            target[\"keypoints\"] = keypoints\n",
    "\n",
    "        # for conversion to coco api\n",
    "        area = torch.tensor([obj[\"area\"] for obj in anno])\n",
    "        iscrowd = torch.tensor([obj[\"iscrowd\"] if \"iscrowd\" in obj else 0 for obj in anno])\n",
    "        target[\"area\"] = area[keep]\n",
    "        target[\"iscrowd\"] = iscrowd[keep]\n",
    "\n",
    "        target[\"orig_size\"] = torch.as_tensor([int(h), int(w)])\n",
    "        target[\"size\"] = torch.as_tensor([int(h), int(w)])\n",
    "\n",
    "        return image, target\n",
    "\n",
    "class CocoDetection(torchvision.datasets.CocoDetection):\n",
    "    def __init__(self, img_folder, ann_file, transforms, return_masks):\n",
    "        super(CocoDetection, self).__init__(img_folder, ann_file)\n",
    "        self._transforms = transforms\n",
    "        self.prepare = ConvertCocoPolysToMask(return_masks)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, target = super(CocoDetection, self).__getitem__(idx)\n",
    "        image_id = self.ids[idx]\n",
    "        target = {'image_id': image_id, 'annotations': target}\n",
    "        img, target = self.prepare(img, target)\n",
    "        if self._transforms is not None:\n",
    "            img, target = self._transforms(img, target)\n",
    "        return img, target\n",
    "\n",
    "def make_coco_transforms(image_set):\n",
    "\n",
    "    normalize = T.Compose([\n",
    "        T.ToTensor(),\n",
    "        T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    scales = [480, 512, 544, 576, 608, 640, 672, 704, 736, 768, 800]\n",
    "\n",
    "    if image_set == 'train':\n",
    "        return T.Compose([\n",
    "            T.RandomHorizontalFlip(),\n",
    "            T.RandomSelect(\n",
    "                T.RandomResize(scales, max_size=1333),\n",
    "                T.Compose([\n",
    "                    T.RandomResize([400, 500, 600]),\n",
    "                    T.RandomSizeCrop(384, 600),\n",
    "                    T.RandomResize(scales, max_size=1333),\n",
    "                ])\n",
    "            ),\n",
    "            normalize,\n",
    "        ])\n",
    "\n",
    "    if image_set == 'val':\n",
    "        return T.Compose([\n",
    "            T.RandomResize([800], max_size=1333),\n",
    "            normalize,\n",
    "        ])\n",
    "\n",
    "    raise ValueError(f'unknown {image_set}')\n",
    "\n",
    "def build(image_set):\n",
    "    mode = 'instances'\n",
    "\n",
    "    PATHS = {\n",
    "        \"train\": (os.path.join('../detr/tiny_coco', \"train2017\"), os.path.join('../detr/tiny_coco', \"annotations\", f'{mode}_train2017.json')),\n",
    "        \"val\": (os.path.join('../detr/tiny_coco', \"val2017\"), os.path.join('../detr/tiny_coco', \"annotations\", f'{mode}_val2017.json')),\n",
    "    }\n",
    "    \n",
    "    img_folder, ann_file = PATHS[image_set]\n",
    "    dataset = CocoDetection(img_folder, ann_file, transforms=make_coco_transforms(image_set), return_masks=False)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, List\n",
    "from torch import Tensor\n",
    "\n",
    "class NestedTensor(object):\n",
    "    def __init__(self, tensors, mask: Optional[Tensor]):\n",
    "        self.tensors = tensors\n",
    "        self.mask = mask\n",
    "\n",
    "    def to(self, device):\n",
    "        # type: (Device) -> NestedTensor # noqa\n",
    "        cast_tensor = self.tensors.to(device)\n",
    "        mask = self.mask\n",
    "        if mask is not None:\n",
    "            assert mask is not None\n",
    "            cast_mask = mask.to(device)\n",
    "        else:\n",
    "            cast_mask = None\n",
    "        return NestedTensor(cast_tensor, cast_mask)\n",
    "\n",
    "    def decompose(self):\n",
    "        return self.tensors, self.mask\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self.tensors)\n",
    "\n",
    "def _max_by_axis(the_list):\n",
    "    # type: (List[List[int]]) -> List[int]\n",
    "    maxes = the_list[0]\n",
    "    for sublist in the_list[1:]:\n",
    "        for index, item in enumerate(sublist):\n",
    "            maxes[index] = max(maxes[index], item)\n",
    "    return maxes\n",
    "\n",
    "\n",
    "def nested_tensor_from_tensor_list(tensor_list: List[Tensor]):\n",
    "    # TODO make this more general\n",
    "    if tensor_list[0].ndim == 3:\n",
    "        \n",
    "        # TODO make it support different-sized images\n",
    "        max_size = _max_by_axis([list(img.shape) for img in tensor_list])\n",
    "        # min_size = tuple(min(s) for s in zip(*[img.shape for img in tensor_list]))\n",
    "        batch_shape = [len(tensor_list)] + max_size\n",
    "        b, c, h, w = batch_shape\n",
    "        dtype = tensor_list[0].dtype\n",
    "        device = tensor_list[0].device\n",
    "        tensor = torch.zeros(batch_shape, dtype=dtype, device=device)\n",
    "        mask = torch.ones((b, h, w), dtype=torch.bool, device=device)\n",
    "        for img, pad_img, m in zip(tensor_list, tensor, mask):\n",
    "            pad_img[: img.shape[0], : img.shape[1], : img.shape[2]].copy_(img)\n",
    "            m[: img.shape[1], :img.shape[2]] = False\n",
    "    else:\n",
    "        raise ValueError('not supported')\n",
    "    return NestedTensor(tensor, mask)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    '''\n",
    "    original dataloader return like: [(image1, annotations1), (image2, annotations2), ...]\n",
    "    \n",
    "    list(zip(*batch)) => [(image1, image2, ...), (annotations1, annotations2, ...)]\n",
    "    \n",
    "    then to  nested_tensor\n",
    "    '''\n",
    "    batch = list(zip(*batch))\n",
    "    batch[0] = nested_tensor_from_tensor_list(batch[0])\n",
    "    return tuple(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "\n",
      "Number of training samples: 25\n",
      "\n",
      "Number of training samples: 25\n"
     ]
    }
   ],
   "source": [
    "train_dataset = build(image_set='train')\n",
    "val_dataset = build(image_set='val')\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=2, collate_fn=collate_fn, num_workers=0, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=2, collate_fn=collate_fn, num_workers=0)\n",
    "\n",
    "print(f'\\nNumber of training samples: {len(train_dataloader)}')\n",
    "print(f'\\nNumber of training samples: {len(val_dataloader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 768, 862])\n",
      "torch.Size([2, 768, 862])\n",
      "{'boxes': tensor([[0.5732, 0.6771, 0.2575, 0.4279],\n",
      "        [0.3253, 0.7910, 0.2207, 0.3910],\n",
      "        [0.1198, 0.5646, 0.2390, 0.1819]]), 'labels': tensor([64, 70, 81]), 'image_id': tensor([144941]), 'area': tensor([23253.4355, 28365.2285, 17129.6621]), 'iscrowd': tensor([0, 0, 0]), 'orig_size': tensor([334, 500]), 'size': tensor([576, 862])}\n",
      "{'boxes': tensor([[0.9730, 0.7950, 0.0539, 0.1824]]), 'labels': tensor([2]), 'image_id': tensor([79841]), 'area': tensor([6389.4028]), 'iscrowd': tensor([0]), 'orig_size': tensor([213, 640]), 'size': tensor([768, 846])}\n"
     ]
    }
   ],
   "source": [
    "# NestedTensor, Tuple of (annotations) => Tuple(Tensors, masks), Tuple of (annotations)\n",
    "samples, targets = next(iter(train_dataloader))\n",
    "\n",
    "print(samples.tensors.shape)\n",
    "print(samples.mask.shape)\n",
    "\n",
    "print(targets[0])\n",
    "print(targets[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class PositionEmbeddingSine(nn.Module):\n",
    "    \"\"\"\n",
    "    This is a more standard version of the position embedding, very similar to the one\n",
    "    used by the Attention is all you need paper, generalized to work on images.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_pos_feats=64, temperature=10000, normalize=False, scale=None):\n",
    "        super().__init__()\n",
    "        self.num_pos_feats = num_pos_feats\n",
    "        self.temperature = temperature\n",
    "        self.normalize = normalize\n",
    "        if scale is not None and normalize is False:\n",
    "            raise ValueError(\"normalize should be True if scale is passed\")\n",
    "        if scale is None:\n",
    "            scale = 2 * math.pi\n",
    "        self.scale = scale\n",
    "\n",
    "    def forward(self, tensor_list: NestedTensor):\n",
    "        x = tensor_list.tensors\n",
    "        mask = tensor_list.mask\n",
    "        assert mask is not None\n",
    "        not_mask = ~mask\n",
    "        y_embed = not_mask.cumsum(1, dtype=torch.float32)\n",
    "        x_embed = not_mask.cumsum(2, dtype=torch.float32)\n",
    "        if self.normalize:\n",
    "            eps = 1e-6\n",
    "            y_embed = y_embed / (y_embed[:, -1:, :] + eps) * self.scale\n",
    "            x_embed = x_embed / (x_embed[:, :, -1:] + eps) * self.scale\n",
    "\n",
    "        dim_t = torch.arange(self.num_pos_feats, dtype=torch.float32, device=x.device)\n",
    "        dim_t = self.temperature ** (2 * (dim_t // 2) / self.num_pos_feats)\n",
    "\n",
    "        pos_x = x_embed[:, :, :, None] / dim_t\n",
    "        pos_y = y_embed[:, :, :, None] / dim_t\n",
    "        pos_x = torch.stack((pos_x[:, :, :, 0::2].sin(), pos_x[:, :, :, 1::2].cos()), dim=4).flatten(3)\n",
    "        pos_y = torch.stack((pos_y[:, :, :, 0::2].sin(), pos_y[:, :, :, 1::2].cos()), dim=4).flatten(3)\n",
    "        pos = torch.cat((pos_y, pos_x), dim=3).permute(0, 3, 1, 2)\n",
    "        return pos\n",
    "\n",
    "\n",
    "class PositionEmbeddingLearned(nn.Module):\n",
    "    \"\"\"\n",
    "    Absolute pos embedding, learned.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_pos_feats=256):\n",
    "        super().__init__()\n",
    "        self.row_embed = nn.Embedding(50, num_pos_feats)\n",
    "        self.col_embed = nn.Embedding(50, num_pos_feats)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.uniform_(self.row_embed.weight)\n",
    "        nn.init.uniform_(self.col_embed.weight)\n",
    "\n",
    "    def forward(self, tensor_list: NestedTensor):\n",
    "        x = tensor_list.tensors\n",
    "        h, w = x.shape[-2:]\n",
    "        i = torch.arange(w, device=x.device)\n",
    "        j = torch.arange(h, device=x.device)\n",
    "        x_emb = self.col_embed(i)\n",
    "        y_emb = self.row_embed(j)\n",
    "        pos = torch.cat([\n",
    "            x_emb.unsqueeze(0).repeat(h, 1, 1),\n",
    "            y_emb.unsqueeze(1).repeat(1, w, 1),\n",
    "        ], dim=-1).permute(2, 0, 1).unsqueeze(0).repeat(x.shape[0], 1, 1, 1)\n",
    "        return pos\n",
    "\n",
    "def build_position_encoding(hidden_dim=256, position_embedding = 'sine'):\n",
    "    N_steps = hidden_dim // 2\n",
    "    if position_embedding in ('v2', 'sine'):\n",
    "        # TODO find a better way of exposing other arguments\n",
    "        position_embedding = PositionEmbeddingSine(N_steps, normalize=True)\n",
    "    elif position_embedding in ('v3', 'learned'):\n",
    "        position_embedding = PositionEmbeddingLearned(N_steps)\n",
    "    else:\n",
    "        raise ValueError(f\"not supported {position_embedding}\")\n",
    "\n",
    "    return position_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models._utils import IntermediateLayerGetter\n",
    "from typing import Dict, List\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FrozenBatchNorm2d(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    BatchNorm2d where the batch statistics and the affine parameters are fixed.\n",
    "\n",
    "    Copy-paste from torchvision.misc.ops with added eps before rqsrt,\n",
    "    without which any other models than torchvision.models.resnet[18,34,50,101]\n",
    "    produce nans.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n):\n",
    "        super(FrozenBatchNorm2d, self).__init__()\n",
    "        self.register_buffer(\"weight\", torch.ones(n))\n",
    "        self.register_buffer(\"bias\", torch.zeros(n))\n",
    "        self.register_buffer(\"running_mean\", torch.zeros(n))\n",
    "        self.register_buffer(\"running_var\", torch.ones(n))\n",
    "\n",
    "    def _load_from_state_dict(self, state_dict, prefix, local_metadata, strict,\n",
    "                              missing_keys, unexpected_keys, error_msgs):\n",
    "        num_batches_tracked_key = prefix + 'num_batches_tracked'\n",
    "        if num_batches_tracked_key in state_dict:\n",
    "            del state_dict[num_batches_tracked_key]\n",
    "\n",
    "        super(FrozenBatchNorm2d, self)._load_from_state_dict(\n",
    "            state_dict, prefix, local_metadata, strict,\n",
    "            missing_keys, unexpected_keys, error_msgs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # move reshapes to the beginning\n",
    "        # to make it fuser-friendly\n",
    "        w = self.weight.reshape(1, -1, 1, 1)\n",
    "        b = self.bias.reshape(1, -1, 1, 1)\n",
    "        rv = self.running_var.reshape(1, -1, 1, 1)\n",
    "        rm = self.running_mean.reshape(1, -1, 1, 1)\n",
    "        eps = 1e-5\n",
    "        scale = w * (rv + eps).rsqrt()\n",
    "        bias = b - rm * scale\n",
    "        return x * scale + bias\n",
    "\n",
    "\n",
    "class BackboneBase(nn.Module):\n",
    "\n",
    "    def __init__(self, backbone: nn.Module, train_backbone: bool, num_channels: int, return_interm_layers: bool):\n",
    "        super().__init__()\n",
    "        for name, parameter in backbone.named_parameters():\n",
    "            if not train_backbone or 'layer2' not in name and 'layer3' not in name and 'layer4' not in name:\n",
    "                parameter.requires_grad_(False)\n",
    "        if return_interm_layers:\n",
    "            return_layers = {\"layer2\": \"layer2\", \"layer3\": \"layer3\", \"layer4\": \"layer4\"}\n",
    "        else:\n",
    "            return_layers = {'layer4': \"layer4\"}\n",
    "        self.body = IntermediateLayerGetter(backbone, return_layers=return_layers)\n",
    "        self.num_channels = num_channels\n",
    "\n",
    "    def forward(self, tensor_list: NestedTensor):\n",
    "        xs = self.body(tensor_list.tensors)\n",
    "        out: Dict[str, NestedTensor] = {}\n",
    "        for name, x in xs.items():\n",
    "            m = tensor_list.mask\n",
    "            assert m is not None\n",
    "            mask = F.interpolate(m[None].float(), size=x.shape[-2:]).to(torch.bool)[0]\n",
    "            out[name] = NestedTensor(x, mask)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Backbone(BackboneBase):\n",
    "    \"\"\"ResNet backbone with frozen BatchNorm.\"\"\"\n",
    "    def __init__(self, name: str,\n",
    "                 train_backbone: bool,\n",
    "                 return_interm_layers: bool,\n",
    "                 dilation: bool):\n",
    "        \n",
    "        replace_stride_with_dilation = [False, False, dilation]\n",
    "        norm_layer = FrozenBatchNorm2d\n",
    "\n",
    "        weights = None\n",
    "        if name == 'resnet18':\n",
    "            weights = torchvision.models.ResNet18_Weights.DEFAULT\n",
    "            backbone = torchvision.models.resnet18(weights=weights, replace_stride_with_dilation=replace_stride_with_dilation, norm_layer=norm_layer)\n",
    "            num_channels = 512\n",
    "        elif name == 'resnet34':\n",
    "            weights = torchvision.models.ResNet34_Weights.DEFAULT\n",
    "            backbone = torchvision.models.resnet34(weights=weights, replace_stride_with_dilation=replace_stride_with_dilation, norm_layer=norm_layer)\n",
    "            num_channels = 512\n",
    "        elif name == 'resnet50':\n",
    "            weights = torchvision.models.ResNet50_Weights.DEFAULT\n",
    "            backbone = torchvision.models.resnet50(weights=weights, replace_stride_with_dilation=replace_stride_with_dilation, norm_layer=norm_layer)\n",
    "            num_channels = 2048\n",
    "        elif name == 'resnet101':\n",
    "            weights = torchvision.models.ResNet101_Weights.DEFAULT\n",
    "            backbone = torchvision.models.resnet101(weights=weights, replace_stride_with_dilation=replace_stride_with_dilation, norm_layer=norm_layer)\n",
    "            num_channels = 2048\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported backbone: {name}\")\n",
    "        \n",
    "        num_channels = 512 if name in ('resnet18', 'resnet34') else 2048\n",
    "        super().__init__(backbone, train_backbone, num_channels, return_interm_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer2 tensor shape : torch.Size([2, 512, 96, 108]), mask shape: torch.Size([2, 96, 108])\n",
      "layer3 tensor shape : torch.Size([2, 1024, 48, 54]), mask shape: torch.Size([2, 48, 54])\n",
      "layer4 tensor shape : torch.Size([2, 2048, 24, 27]), mask shape: torch.Size([2, 24, 27])\n"
     ]
    }
   ],
   "source": [
    "backbone = Backbone(name='resnet50', train_backbone=False, return_interm_layers=True, dilation=False)\n",
    "\n",
    "# Dict of {layer_{i}: nestedTensor}\n",
    "backbone_out_nestedTensor = backbone(samples)\n",
    "\n",
    "for k, v in backbone_out_nestedTensor.items():\n",
    "    print(f\"{k} tensor shape : {v.tensors.shape}, mask shape: {v.mask.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HybridEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import inspect\n",
    "import importlib\n",
    "\n",
    "GLOBAL_CONFIG = dict()\n",
    "\n",
    "def extract_schema(cls: type):\n",
    "    '''\n",
    "    Args:\n",
    "        cls (type),\n",
    "    Return:\n",
    "        Dict, \n",
    "    '''\n",
    "    argspec = inspect.getfullargspec(cls.__init__)\n",
    "    arg_names = [arg for arg in argspec.args if arg != 'self']\n",
    "    num_defualts = len(argspec.defaults) if argspec.defaults is not None else 0\n",
    "    num_requires = len(arg_names) - num_defualts\n",
    "\n",
    "    schame = dict()\n",
    "    schame['_name'] = cls.__name__\n",
    "    schame['_pymodule'] = importlib.import_module(cls.__module__)\n",
    "    schame['_inject'] = getattr(cls, '__inject__', [])\n",
    "    schame['_share'] = getattr(cls, '__share__', [])\n",
    "\n",
    "    for i, name in enumerate(arg_names):\n",
    "        if name in schame['_share']:\n",
    "            assert i >= num_requires, 'share config must have default value.'\n",
    "            value = argspec.defaults[i - num_requires]\n",
    "        \n",
    "        elif i >= num_requires:\n",
    "            value = argspec.defaults[i - num_requires]\n",
    "\n",
    "        else:\n",
    "            value = None \n",
    "\n",
    "        schame[name] = value\n",
    "        \n",
    "    return schame\n",
    "\n",
    "def register(cls: type):\n",
    "    '''\n",
    "    Args:\n",
    "        cls (type): Module class to be registered.\n",
    "    '''\n",
    "    if cls.__name__ in GLOBAL_CONFIG:\n",
    "        raise ValueError('{} already registered'.format(cls.__name__))\n",
    "\n",
    "    if inspect.isfunction(cls):\n",
    "        GLOBAL_CONFIG[cls.__name__] = cls\n",
    "    \n",
    "    elif inspect.isclass(cls):\n",
    "        GLOBAL_CONFIG[cls.__name__] = extract_schema(cls)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f'register {cls}')\n",
    "\n",
    "    return cls \n",
    "\n",
    "def get_activation(act: str, inpace: bool=True):\n",
    "    '''get activation\n",
    "    '''\n",
    "    act = act.lower()\n",
    "    \n",
    "    if act == 'silu':\n",
    "        m = nn.SiLU()\n",
    "\n",
    "    elif act == 'relu':\n",
    "        m = nn.ReLU()\n",
    "\n",
    "    elif act == 'leaky_relu':\n",
    "        m = nn.LeakyReLU()\n",
    "\n",
    "    elif act == 'silu':\n",
    "        m = nn.SiLU()\n",
    "    \n",
    "    elif act == 'gelu':\n",
    "        m = nn.GELU()\n",
    "        \n",
    "    elif act is None:\n",
    "        m = nn.Identity()\n",
    "    \n",
    "    elif isinstance(act, nn.Module):\n",
    "        m = act\n",
    "\n",
    "    else:\n",
    "        raise RuntimeError('')  \n",
    "\n",
    "    if hasattr(m, 'inplace'):\n",
    "        m.inplace = inpace\n",
    "    \n",
    "    return m \n",
    "\n",
    "class ConvNormLayer(nn.Module):\n",
    "    def __init__(self, ch_in, ch_out, kernel_size, stride, padding=None, bias=False, act=None):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(\n",
    "            ch_in, \n",
    "            ch_out, \n",
    "            kernel_size, \n",
    "            stride, \n",
    "            padding=(kernel_size-1)//2 if padding is None else padding, \n",
    "            bias=bias)\n",
    "        self.norm = nn.BatchNorm2d(ch_out)\n",
    "        self.act = nn.Identity() if act is None else get_activation(act) \n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.act(self.norm(self.conv(x)))\n",
    "\n",
    "\n",
    "class RepVggBlock(nn.Module):\n",
    "    def __init__(self, ch_in, ch_out, act='relu'):\n",
    "        super().__init__()\n",
    "        self.ch_in = ch_in\n",
    "        self.ch_out = ch_out\n",
    "        self.conv1 = ConvNormLayer(ch_in, ch_out, 3, 1, padding=1, act=None)\n",
    "        self.conv2 = ConvNormLayer(ch_in, ch_out, 1, 1, padding=0, act=None)\n",
    "        self.act = nn.Identity() if act is None else get_activation(act) \n",
    "\n",
    "    def forward(self, x):\n",
    "        if hasattr(self, 'conv'):\n",
    "            y = self.conv(x)\n",
    "        else:\n",
    "            y = self.conv1(x) + self.conv2(x)\n",
    "\n",
    "        return self.act(y)\n",
    "\n",
    "    def convert_to_deploy(self):\n",
    "        if not hasattr(self, 'conv'):\n",
    "            self.conv = nn.Conv2d(self.ch_in, self.ch_out, 3, 1, padding=1)\n",
    "\n",
    "        kernel, bias = self.get_equivalent_kernel_bias()\n",
    "        self.conv.weight.data = kernel\n",
    "        self.conv.bias.data = bias \n",
    "        # self.__delattr__('conv1')\n",
    "        # self.__delattr__('conv2')\n",
    "\n",
    "    def get_equivalent_kernel_bias(self):\n",
    "        kernel3x3, bias3x3 = self._fuse_bn_tensor(self.conv1)\n",
    "        kernel1x1, bias1x1 = self._fuse_bn_tensor(self.conv2)\n",
    "        \n",
    "        return kernel3x3 + self._pad_1x1_to_3x3_tensor(kernel1x1), bias3x3 + bias1x1\n",
    "\n",
    "    def _pad_1x1_to_3x3_tensor(self, kernel1x1):\n",
    "        if kernel1x1 is None:\n",
    "            return 0\n",
    "        else:\n",
    "            return F.pad(kernel1x1, [1, 1, 1, 1])\n",
    "\n",
    "    def _fuse_bn_tensor(self, branch: ConvNormLayer):\n",
    "        if branch is None:\n",
    "            return 0, 0\n",
    "        kernel = branch.conv.weight\n",
    "        running_mean = branch.norm.running_mean\n",
    "        running_var = branch.norm.running_var\n",
    "        gamma = branch.norm.weight\n",
    "        beta = branch.norm.bias\n",
    "        eps = branch.norm.eps\n",
    "        std = (running_var + eps).sqrt()\n",
    "        t = (gamma / std).reshape(-1, 1, 1, 1)\n",
    "        return kernel * t, beta - running_mean * gamma / std\n",
    "\n",
    "\n",
    "class CSPRepLayer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 num_blocks=3,\n",
    "                 expansion=1.0,\n",
    "                 bias=None,\n",
    "                 act=\"silu\"):\n",
    "        super(CSPRepLayer, self).__init__()\n",
    "        hidden_channels = int(out_channels * expansion)\n",
    "        self.conv1 = ConvNormLayer(in_channels, hidden_channels, 1, 1, bias=bias, act=act)\n",
    "        self.conv2 = ConvNormLayer(in_channels, hidden_channels, 1, 1, bias=bias, act=act)\n",
    "        self.bottlenecks = nn.Sequential(*[\n",
    "            RepVggBlock(hidden_channels, hidden_channels, act=act) for _ in range(num_blocks)\n",
    "        ])\n",
    "        if hidden_channels != out_channels:\n",
    "            self.conv3 = ConvNormLayer(hidden_channels, out_channels, 1, 1, bias=bias, act=act)\n",
    "        else:\n",
    "            self.conv3 = nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_1 = self.conv1(x)\n",
    "        x_1 = self.bottlenecks(x_1)\n",
    "        x_2 = self.conv2(x)\n",
    "        return self.conv3(x_1 + x_2)\n",
    "\n",
    "# transformer\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 d_model,\n",
    "                 nhead,\n",
    "                 dim_feedforward=2048,\n",
    "                 dropout=0.1,\n",
    "                 activation=\"relu\",\n",
    "                 normalize_before=False):\n",
    "        super().__init__()\n",
    "        self.normalize_before = normalize_before\n",
    "\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout, batch_first=True)\n",
    "\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.activation = get_activation(activation) \n",
    "\n",
    "    @staticmethod\n",
    "    def with_pos_embed(tensor, pos_embed):\n",
    "        return tensor if pos_embed is None else tensor + pos_embed\n",
    "\n",
    "    def forward(self, src, src_mask=None, pos_embed=None) -> torch.Tensor:\n",
    "        residual = src\n",
    "        if self.normalize_before:\n",
    "            src = self.norm1(src)\n",
    "        q = k = self.with_pos_embed(src, pos_embed)\n",
    "        src, _ = self.self_attn(q, k, value=src, attn_mask=src_mask)\n",
    "\n",
    "        src = residual + self.dropout1(src)\n",
    "        if not self.normalize_before:\n",
    "            src = self.norm1(src)\n",
    "\n",
    "        residual = src\n",
    "        if self.normalize_before:\n",
    "            src = self.norm2(src)\n",
    "        src = self.linear2(self.dropout(self.activation(self.linear1(src))))\n",
    "        src = residual + self.dropout2(src)\n",
    "        if not self.normalize_before:\n",
    "            src = self.norm2(src)\n",
    "        return src\n",
    "\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, encoder_layer, num_layers, norm=None):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.layers = nn.ModuleList([copy.deepcopy(encoder_layer) for _ in range(num_layers)])\n",
    "        self.num_layers = num_layers\n",
    "        self.norm = norm\n",
    "\n",
    "    def forward(self, src, src_mask=None, pos_embed=None) -> torch.Tensor:\n",
    "        output = src\n",
    "        for layer in self.layers:\n",
    "            output = layer(output, src_mask=src_mask, pos_embed=pos_embed)\n",
    "\n",
    "        if self.norm is not None:\n",
    "            output = self.norm(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "@register\n",
    "class HybridEncoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels=[512, 1024, 2048],\n",
    "                 feat_strides=[8, 16, 32],\n",
    "                 hidden_dim=256,\n",
    "                 nhead=8,\n",
    "                 dim_feedforward = 1024,\n",
    "                 dropout=0.0,\n",
    "                 enc_act='gelu',\n",
    "                 use_encoder_idx=[2],\n",
    "                 num_encoder_layers=1,\n",
    "                 pe_temperature=10000,\n",
    "                 expansion=1.0,\n",
    "                 depth_mult=1.0,\n",
    "                 act='silu',\n",
    "                 eval_spatial_size=None):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.feat_strides = feat_strides\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.use_encoder_idx = use_encoder_idx\n",
    "        self.num_encoder_layers = num_encoder_layers\n",
    "        self.pe_temperature = pe_temperature\n",
    "        self.eval_spatial_size = eval_spatial_size\n",
    "\n",
    "        self.out_channels = [hidden_dim for _ in range(len(in_channels))]\n",
    "        self.out_strides = feat_strides\n",
    "        \n",
    "        # channel projection\n",
    "        self.input_proj = nn.ModuleList()\n",
    "        for in_channel in in_channels:\n",
    "            self.input_proj.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(in_channel, hidden_dim, kernel_size=1, bias=False),\n",
    "                    nn.BatchNorm2d(hidden_dim)\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # encoder transformer\n",
    "        encoder_layer = TransformerEncoderLayer(\n",
    "            hidden_dim, \n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward, \n",
    "            dropout=dropout,\n",
    "            activation=enc_act)\n",
    "\n",
    "        self.encoder = nn.ModuleList([\n",
    "            TransformerEncoder(copy.deepcopy(encoder_layer), num_encoder_layers) for _ in range(len(use_encoder_idx))\n",
    "        ])\n",
    "\n",
    "        # top-down fpn\n",
    "        self.lateral_convs = nn.ModuleList()\n",
    "        self.fpn_blocks = nn.ModuleList()\n",
    "        for _ in range(len(in_channels) - 1, 0, -1):\n",
    "            self.lateral_convs.append(ConvNormLayer(hidden_dim, hidden_dim, 1, 1, act=act))\n",
    "            self.fpn_blocks.append(\n",
    "                CSPRepLayer(hidden_dim * 2, hidden_dim, round(3 * depth_mult), act=act, expansion=expansion)\n",
    "            )\n",
    "\n",
    "        # bottom-up pan\n",
    "        self.downsample_convs = nn.ModuleList()\n",
    "        self.pan_blocks = nn.ModuleList()\n",
    "        for _ in range(len(in_channels) - 1):\n",
    "            self.downsample_convs.append(\n",
    "                ConvNormLayer(hidden_dim, hidden_dim, 3, 2, act=act)\n",
    "            )\n",
    "            self.pan_blocks.append(\n",
    "                CSPRepLayer(hidden_dim * 2, hidden_dim, round(3 * depth_mult), act=act, expansion=expansion)\n",
    "            )\n",
    "\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        if self.eval_spatial_size:\n",
    "            for idx in self.use_encoder_idx:\n",
    "                stride = self.feat_strides[idx]\n",
    "                pos_embed = self.build_2d_sincos_position_embedding(\n",
    "                    self.eval_spatial_size[1] // stride, self.eval_spatial_size[0] // stride,\n",
    "                    self.hidden_dim, self.pe_temperature)\n",
    "                setattr(self, f'pos_embed{idx}', pos_embed)\n",
    "                # self.register_buffer(f'pos_embed{idx}', pos_embed)\n",
    "\n",
    "    @staticmethod\n",
    "    def build_2d_sincos_position_embedding(w, h, embed_dim=256, temperature=10000.):\n",
    "        '''\n",
    "        '''\n",
    "        grid_w = torch.arange(int(w), dtype=torch.float32)\n",
    "        grid_h = torch.arange(int(h), dtype=torch.float32)\n",
    "        grid_w, grid_h = torch.meshgrid(grid_w, grid_h, indexing='ij')\n",
    "        assert embed_dim % 4 == 0, \\\n",
    "            'Embed dimension must be divisible by 4 for 2D sin-cos position embedding'\n",
    "        pos_dim = embed_dim // 4\n",
    "        omega = torch.arange(pos_dim, dtype=torch.float32) / pos_dim\n",
    "        omega = 1. / (temperature ** omega)\n",
    "\n",
    "        out_w = grid_w.flatten()[..., None] @ omega[None]\n",
    "        out_h = grid_h.flatten()[..., None] @ omega[None]\n",
    "\n",
    "        return torch.concat([out_w.sin(), out_w.cos(), out_h.sin(), out_h.cos()], dim=1)[None, :, :]\n",
    "\n",
    "    def forward(self, feats):\n",
    "        assert len(feats) == len(self.in_channels)\n",
    "        proj_feats = [self.input_proj[i](feat) for i, feat in enumerate(feats)]\n",
    "        \n",
    "        # encoder\n",
    "        if self.num_encoder_layers > 0:\n",
    "            for i, enc_ind in enumerate(self.use_encoder_idx):\n",
    "                h, w = proj_feats[enc_ind].shape[2:]\n",
    "                # flatten [B, C, H, W] to [B, HxW, C]\n",
    "                src_flatten = proj_feats[enc_ind].flatten(2).permute(0, 2, 1)\n",
    "                if self.training or self.eval_spatial_size is None:\n",
    "                    pos_embed = self.build_2d_sincos_position_embedding(\n",
    "                        w, h, self.hidden_dim, self.pe_temperature).to(src_flatten.device)\n",
    "                else:\n",
    "                    pos_embed = getattr(self, f'pos_embed{enc_ind}', None).to(src_flatten.device)\n",
    "\n",
    "                memory = self.encoder[i](src_flatten, pos_embed=pos_embed)\n",
    "                proj_feats[enc_ind] = memory.permute(0, 2, 1).reshape(-1, self.hidden_dim, h, w).contiguous()\n",
    "                # print([x.is_contiguous() for x in proj_feats ])\n",
    "\n",
    "        # broadcasting and fusion\n",
    "        inner_outs = [proj_feats[-1]]\n",
    "        for idx in range(len(self.in_channels) - 1, 0, -1):\n",
    "            feat_high = inner_outs[0]\n",
    "            feat_low = proj_feats[idx - 1]\n",
    "            feat_high = self.lateral_convs[len(self.in_channels) - 1 - idx](feat_high)\n",
    "            inner_outs[0] = feat_high\n",
    "            upsample_feat = F.interpolate(feat_high, scale_factor=2., mode='nearest')\n",
    "            inner_out = self.fpn_blocks[len(self.in_channels)-1-idx](torch.concat([upsample_feat, feat_low], dim=1))\n",
    "            inner_outs.insert(0, inner_out)\n",
    "\n",
    "        outs = [inner_outs[0]]\n",
    "        for idx in range(len(self.in_channels) - 1):\n",
    "            feat_low = outs[-1]\n",
    "            feat_high = inner_outs[idx + 1]\n",
    "            downsample_feat = self.downsample_convs[idx](feat_low)\n",
    "            out = self.pan_blocks[idx](torch.concat([downsample_feat, feat_high], dim=1))\n",
    "            outs.append(out)\n",
    "\n",
    "        return outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer2 tensor shape : torch.Size([2, 512, 96, 108]), mask shape: torch.Size([2, 96, 108])\n",
      "layer3 tensor shape : torch.Size([2, 1024, 48, 54]), mask shape: torch.Size([2, 48, 54])\n",
      "layer4 tensor shape : torch.Size([2, 2048, 24, 27]), mask shape: torch.Size([2, 24, 27])\n",
      "torch.Size([2, 512, 96, 108])\n",
      "torch.Size([2, 1024, 48, 54])\n",
      "torch.Size([2, 2048, 24, 27])\n",
      "<class 'list'>\n",
      "list_0 shape: torch.Size([2, 256, 96, 108])\n",
      "list_1 shape: torch.Size([2, 256, 48, 54])\n",
      "list_2 shape: torch.Size([2, 256, 24, 27])\n"
     ]
    }
   ],
   "source": [
    "model = HybridEncoder()\n",
    "\n",
    "list_tensors = []\n",
    "\n",
    "for k, v in backbone_out_nestedTensor.items():\n",
    "    print(f\"{k} tensor shape : {v.tensors.shape}, mask shape: {v.mask.shape}\")\n",
    "    list_tensors.append(v.tensors)\n",
    "\n",
    "for i in list_tensors:\n",
    "    print(i.shape)\n",
    "\n",
    "out = model(list_tensors)\n",
    "    \n",
    "print(type(out))\n",
    "\n",
    "for i, l in enumerate(out):\n",
    "        print(f'list_{i} shape: {l.shape}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
