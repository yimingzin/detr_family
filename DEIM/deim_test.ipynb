{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97d38382",
   "metadata": {},
   "source": [
    "# backbone: PResNet or HGnetv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10d5e4e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 384, 80, 80])\n",
      "torch.Size([2, 768, 40, 40])\n",
      "torch.Size([2, 1536, 20, 20])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "S3 = torch.rand(size=(2, 384, 80, 80))\n",
    "S4 = torch.rand(size=(2, 768, 40, 40))\n",
    "S5 = torch.rand(size=(2, 1536, 20, 20))\n",
    "\n",
    "backbone_out = [S3, S4, S5]\n",
    "\n",
    "for i in backbone_out:\n",
    "    print(i.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1d861a",
   "metadata": {},
   "source": [
    "# Hybrid Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e863ca28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activation(act: str, inpace: bool=True):\n",
    "    \"\"\"get activation\n",
    "    \"\"\"\n",
    "    if act is None:\n",
    "        return nn.Identity()\n",
    "\n",
    "    elif isinstance(act, nn.Module):\n",
    "        return act\n",
    "\n",
    "    act = act.lower()\n",
    "\n",
    "    if act == 'silu' or act == 'swish':\n",
    "        m = nn.SiLU()\n",
    "\n",
    "    elif act == 'relu':\n",
    "        m = nn.ReLU()\n",
    "\n",
    "    elif act == 'leaky_relu':\n",
    "        m = nn.LeakyReLU()\n",
    "\n",
    "    elif act == 'silu':\n",
    "        m = nn.SiLU()\n",
    "\n",
    "    elif act == 'gelu':\n",
    "        m = nn.GELU()\n",
    "\n",
    "    elif act == 'hardsigmoid':\n",
    "        m = nn.Hardsigmoid()\n",
    "\n",
    "    else:\n",
    "        raise RuntimeError('')\n",
    "\n",
    "    if hasattr(m, 'inplace'):\n",
    "        m.inplace = inpace\n",
    "\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3598444",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNormLayer_fuse(nn.Module):\n",
    "    def __init__(self, ch_in, ch_out, kernel_size, stride, g=1, padding=None, bias=False, act=None):\n",
    "        super().__init__()\n",
    "        padding = (kernel_size-1)//2 if padding is None else padding\n",
    "        self.conv = nn.Conv2d(\n",
    "            ch_in,\n",
    "            ch_out,\n",
    "            kernel_size,\n",
    "            stride,\n",
    "            groups=g,\n",
    "            padding=padding,\n",
    "            bias=bias)\n",
    "        self.norm = nn.BatchNorm2d(ch_out)\n",
    "        self.act = nn.Identity() if act is None else get_activation(act)\n",
    "        self.ch_in, self.ch_out, self.kernel_size, self.stride, self.g, self.padding, self.bias = \\\n",
    "            ch_in, ch_out, kernel_size, stride, g, padding, bias\n",
    "\n",
    "    def forward(self, x):\n",
    "        if hasattr(self, 'conv_bn_fused'):\n",
    "            y = self.conv_bn_fused(x)\n",
    "        else:\n",
    "            y = self.norm(self.conv(x))\n",
    "        return self.act(y)\n",
    "\n",
    "    def convert_to_deploy(self):\n",
    "        if not hasattr(self, 'conv_bn_fused'):\n",
    "            self.conv_bn_fused = nn.Conv2d(\n",
    "                self.ch_in,\n",
    "                self.ch_out,\n",
    "                self.kernel_size,\n",
    "                self.stride,\n",
    "                groups=self.g,\n",
    "                padding=self.padding,\n",
    "                bias=True)\n",
    "\n",
    "        kernel, bias = self.get_equivalent_kernel_bias()\n",
    "        self.conv_bn_fused.weight.data = kernel\n",
    "        self.conv_bn_fused.bias.data = bias\n",
    "        self.__delattr__('conv')\n",
    "        self.__delattr__('norm')\n",
    "\n",
    "    def get_equivalent_kernel_bias(self):\n",
    "        kernel3x3, bias3x3 = self._fuse_bn_tensor()\n",
    "\n",
    "        return kernel3x3, bias3x3\n",
    "\n",
    "    def _fuse_bn_tensor(self):\n",
    "        kernel = self.conv.weight\n",
    "        running_mean = self.norm.running_mean\n",
    "        running_var = self.norm.running_var\n",
    "        gamma = self.norm.weight\n",
    "        beta = self.norm.bias\n",
    "        eps = self.norm.eps\n",
    "        std = (running_var + eps).sqrt()\n",
    "        t = (gamma / std).reshape(-1, 1, 1, 1)\n",
    "        return kernel * t, beta - running_mean * gamma / std\n",
    "\n",
    "\n",
    "class ConvNormLayer(nn.Module):\n",
    "    def __init__(self, ch_in, ch_out, kernel_size, stride, g=1, padding=None, bias=False, act=None):\n",
    "        super().__init__()\n",
    "        padding = (kernel_size-1)//2 if padding is None else padding\n",
    "        self.conv = nn.Conv2d(\n",
    "            ch_in,\n",
    "            ch_out,\n",
    "            kernel_size,\n",
    "            stride,\n",
    "            groups=g,\n",
    "            padding=padding,\n",
    "            bias=bias)\n",
    "        self.norm = nn.BatchNorm2d(ch_out)\n",
    "        self.act = nn.Identity() if act is None else get_activation(act)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.act(self.norm(self.conv(x)))\n",
    "    \n",
    "# TODO, add activation for cv1 following YOLOv10\n",
    "# self.cv1 = Conv(c1, c2, 1, 1)\n",
    "# self.cv2 = Conv(c2, c2, k=k, s=s, g=c2, act=False)\n",
    "class SCDown(nn.Module):\n",
    "    def __init__(self, c1, c2, k, s, act=None):\n",
    "        super().__init__()\n",
    "        self.cv1 = ConvNormLayer_fuse(c1, c2, 1, 1)\n",
    "        self.cv2 = ConvNormLayer_fuse(c2, c2, k, s, c2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.cv2(self.cv1(x))\n",
    "\n",
    "\n",
    "class VGGBlock(nn.Module):\n",
    "    def __init__(self, ch_in, ch_out, act='relu'):\n",
    "        super().__init__()\n",
    "        self.ch_in = ch_in\n",
    "        self.ch_out = ch_out\n",
    "        self.conv1 = ConvNormLayer(ch_in, ch_out, 3, 1, padding=1, act=None)\n",
    "        self.conv2 = ConvNormLayer(ch_in, ch_out, 1, 1, padding=0, act=None)\n",
    "        self.act = nn.Identity() if act is None else get_activation(act)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if hasattr(self, 'conv'):\n",
    "            y = self.conv(x)\n",
    "        else:\n",
    "            y = self.conv1(x) + self.conv2(x)\n",
    "\n",
    "        return self.act(y)\n",
    "\n",
    "    def convert_to_deploy(self):\n",
    "        if not hasattr(self, 'conv'):\n",
    "            self.conv = nn.Conv2d(self.ch_in, self.ch_out, 3, 1, padding=1)\n",
    "\n",
    "        kernel, bias = self.get_equivalent_kernel_bias()\n",
    "        self.conv.weight.data = kernel\n",
    "        self.conv.bias.data = bias\n",
    "        self.__delattr__('conv1')\n",
    "        self.__delattr__('conv2')\n",
    "\n",
    "    def get_equivalent_kernel_bias(self):\n",
    "        kernel3x3, bias3x3 = self._fuse_bn_tensor(self.conv1)\n",
    "        kernel1x1, bias1x1 = self._fuse_bn_tensor(self.conv2)\n",
    "\n",
    "        return kernel3x3 + self._pad_1x1_to_3x3_tensor(kernel1x1), bias3x3 + bias1x1\n",
    "\n",
    "    def _pad_1x1_to_3x3_tensor(self, kernel1x1):\n",
    "        if kernel1x1 is None:\n",
    "            return 0\n",
    "        else:\n",
    "            return F.pad(kernel1x1, [1, 1, 1, 1])\n",
    "\n",
    "    def _fuse_bn_tensor(self, branch: ConvNormLayer):\n",
    "        if branch is None:\n",
    "            return 0, 0\n",
    "        kernel = branch.conv.weight\n",
    "        running_mean = branch.norm.running_mean\n",
    "        running_var = branch.norm.running_var\n",
    "        gamma = branch.norm.weight\n",
    "        beta = branch.norm.bias\n",
    "        eps = branch.norm.eps\n",
    "        std = (running_var + eps).sqrt()\n",
    "        t = (gamma / std).reshape(-1, 1, 1, 1)\n",
    "        return kernel * t, beta - running_mean * gamma / std\n",
    "\n",
    "\n",
    "class CSPLayer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 num_blocks=3,\n",
    "                 expansion=1.0,\n",
    "                 bias=False,\n",
    "                 act=\"silu\",\n",
    "                 bottletype=VGGBlock):\n",
    "        super(CSPLayer, self).__init__()\n",
    "        hidden_channels = int(out_channels * expansion)\n",
    "        self.conv1 = ConvNormLayer_fuse(in_channels, hidden_channels, 1, 1, bias=bias, act=act)\n",
    "        self.conv2 = ConvNormLayer_fuse(in_channels, hidden_channels, 1, 1, bias=bias, act=act)\n",
    "        self.bottlenecks = nn.Sequential(*[\n",
    "            bottletype(hidden_channels, hidden_channels, act=act) for _ in range(num_blocks)\n",
    "        ])\n",
    "        if hidden_channels != out_channels:\n",
    "            self.conv3 = ConvNormLayer_fuse(hidden_channels, out_channels, 1, 1, bias=bias, act=act)\n",
    "        else:\n",
    "            self.conv3 = nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_2 = self.conv2(x)\n",
    "        x_1 = self.conv1(x)\n",
    "        x_1 = self.bottlenecks(x_1)\n",
    "        return self.conv3(x_1 + x_2)\n",
    "\n",
    "class RepNCSPELAN4(nn.Module):\n",
    "    # csp-elan\n",
    "    def __init__(self, c1, c2, c3, c4, n=3,\n",
    "                 bias=False,\n",
    "                 act=\"silu\"):\n",
    "        super().__init__()\n",
    "        self.c = c3//2\n",
    "        self.cv1 = ConvNormLayer_fuse(c1, c3, 1, 1, bias=bias, act=act)\n",
    "        self.cv2 = nn.Sequential(CSPLayer(c3//2, c4, n, 1, bias=bias, act=act, bottletype=VGGBlock), ConvNormLayer_fuse(c4, c4, 3, 1, bias=bias, act=act))\n",
    "        self.cv3 = nn.Sequential(CSPLayer(c4, c4, n, 1, bias=bias, act=act, bottletype=VGGBlock), ConvNormLayer_fuse(c4, c4, 3, 1, bias=bias, act=act))\n",
    "        self.cv4 = ConvNormLayer_fuse(c3+(2*c4), c2, 1, 1, bias=bias, act=act)\n",
    "\n",
    "    def forward_chunk(self, x):\n",
    "        y = list(self.cv1(x).chunk(2, 1))\n",
    "        y.extend((m(y[-1])) for m in [self.cv2, self.cv3])\n",
    "        return self.cv4(torch.cat(y, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = list(self.cv1(x).split((self.c, self.c), 1))\n",
    "        y.extend(m(y[-1]) for m in [self.cv2, self.cv3])\n",
    "        return self.cv4(torch.cat(y, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a1dc96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "# transformer\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 d_model,\n",
    "                 nhead,\n",
    "                 dim_feedforward=2048,\n",
    "                 dropout=0.1,\n",
    "                 activation=\"relu\",\n",
    "                 normalize_before=False):\n",
    "        super().__init__()\n",
    "        self.normalize_before = normalize_before\n",
    "\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout, batch_first=True)\n",
    "\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.activation = get_activation(activation)\n",
    "\n",
    "    @staticmethod\n",
    "    def with_pos_embed(tensor, pos_embed):\n",
    "        return tensor if pos_embed is None else tensor + pos_embed\n",
    "\n",
    "    def forward(self, src, src_mask=None, pos_embed=None) -> torch.Tensor:\n",
    "        residual = src\n",
    "        if self.normalize_before:\n",
    "            src = self.norm1(src)\n",
    "        q = k = self.with_pos_embed(src, pos_embed)\n",
    "        src, _ = self.self_attn(q, k, value=src, attn_mask=src_mask)\n",
    "\n",
    "        src = residual + self.dropout1(src)\n",
    "        if not self.normalize_before:\n",
    "            src = self.norm1(src)\n",
    "\n",
    "        residual = src\n",
    "        if self.normalize_before:\n",
    "            src = self.norm2(src)\n",
    "        src = self.linear2(self.dropout(self.activation(self.linear1(src))))\n",
    "        src = residual + self.dropout2(src)\n",
    "        if not self.normalize_before:\n",
    "            src = self.norm2(src)\n",
    "        return src\n",
    "\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, encoder_layer, num_layers, norm=None):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.layers = nn.ModuleList([copy.deepcopy(encoder_layer) for _ in range(num_layers)])\n",
    "        self.num_layers = num_layers\n",
    "        self.norm = norm\n",
    "\n",
    "    def forward(self, src, src_mask=None, pos_embed=None) -> torch.Tensor:\n",
    "        output = src\n",
    "        for layer in self.layers:\n",
    "            output = layer(output, src_mask=src_mask, pos_embed=pos_embed)\n",
    "\n",
    "        if self.norm is not None:\n",
    "            output = self.norm(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9084868",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class HybridEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels=[512, 1024, 2048],\n",
    "                 feat_strides=[8, 16, 32],\n",
    "                 hidden_dim=256,\n",
    "                 nhead=8,\n",
    "                 dim_feedforward = 1024,\n",
    "                 dropout=0.0,\n",
    "                 enc_act='gelu',\n",
    "                 use_encoder_idx=[2],\n",
    "                 num_encoder_layers=1,\n",
    "                 pe_temperature=10000,\n",
    "                 expansion=1.0,\n",
    "                 depth_mult=1.0,\n",
    "                 act='silu',\n",
    "                 eval_spatial_size=None,\n",
    "                 version='dfine',\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.feat_strides = feat_strides\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.use_encoder_idx = use_encoder_idx\n",
    "        self.num_encoder_layers = num_encoder_layers\n",
    "        self.pe_temperature = pe_temperature\n",
    "        self.eval_spatial_size = eval_spatial_size\n",
    "        self.out_channels = [hidden_dim for _ in range(len(in_channels))]\n",
    "        self.out_strides = feat_strides\n",
    "\n",
    "        # channel projection\n",
    "        self.input_proj = nn.ModuleList()\n",
    "        for in_channel in in_channels:\n",
    "            proj = nn.Sequential(OrderedDict([\n",
    "                    ('conv', nn.Conv2d(in_channel, hidden_dim, kernel_size=1, bias=False)),\n",
    "                    ('norm', nn.BatchNorm2d(hidden_dim))\n",
    "                ]))\n",
    "\n",
    "            self.input_proj.append(proj)\n",
    "\n",
    "        # encoder transformer\n",
    "        encoder_layer = TransformerEncoderLayer(\n",
    "            hidden_dim,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            activation=enc_act\n",
    "            )\n",
    "\n",
    "        self.encoder = nn.ModuleList([\n",
    "            TransformerEncoder(copy.deepcopy(encoder_layer), num_encoder_layers) for _ in range(len(use_encoder_idx))\n",
    "        ])\n",
    "\n",
    "        # top-down fpn\n",
    "        self.lateral_convs = nn.ModuleList()\n",
    "        self.fpn_blocks = nn.ModuleList()\n",
    "        for _ in range(len(in_channels) - 1, 0, -1):\n",
    "            # TODO, add activation for those lateral convs\n",
    "            if version == 'dfine':\n",
    "                self.lateral_convs.append(ConvNormLayer_fuse(hidden_dim, hidden_dim, 1, 1))\n",
    "            else:\n",
    "                self.lateral_convs.append(ConvNormLayer_fuse(hidden_dim, hidden_dim, 1, 1, act=act))\n",
    "            self.fpn_blocks.append(\n",
    "                RepNCSPELAN4(hidden_dim * 2, hidden_dim, hidden_dim * 2, round(expansion * hidden_dim // 2), round(3 * depth_mult), act=act) \\\n",
    "                if version == 'dfine' else CSPLayer(hidden_dim * 2, hidden_dim, round(3 * depth_mult), act=act, expansion=expansion, bottletype=VGGBlock)\n",
    "            )\n",
    "\n",
    "        # bottom-up pan\n",
    "        self.downsample_convs = nn.ModuleList()\n",
    "        self.pan_blocks = nn.ModuleList()\n",
    "        for _ in range(len(in_channels) - 1):\n",
    "            self.downsample_convs.append(\n",
    "                nn.Sequential(SCDown(hidden_dim, hidden_dim, 3, 2, act=act)) \\\n",
    "                if version == 'dfine' else ConvNormLayer_fuse(hidden_dim, hidden_dim, 3, 2, act=act)\n",
    "            )\n",
    "            self.pan_blocks.append(\n",
    "                RepNCSPELAN4(hidden_dim * 2, hidden_dim, hidden_dim * 2, round(expansion * hidden_dim // 2), round(3 * depth_mult), act=act) \\\n",
    "                if version == 'dfine' else CSPLayer(hidden_dim * 2, hidden_dim, round(3 * depth_mult), act=act, expansion=expansion, bottletype=VGGBlock)\n",
    "            )\n",
    "\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        if self.eval_spatial_size:\n",
    "            for idx in self.use_encoder_idx:\n",
    "                stride = self.feat_strides[idx]\n",
    "                pos_embed = self.build_2d_sincos_position_embedding(\n",
    "                    self.eval_spatial_size[1] // stride, self.eval_spatial_size[0] // stride,\n",
    "                    self.hidden_dim, self.pe_temperature)\n",
    "                setattr(self, f'pos_embed{idx}', pos_embed)\n",
    "                # self.register_buffer(f'pos_embed{idx}', pos_embed)\n",
    "\n",
    "    @staticmethod\n",
    "    def build_2d_sincos_position_embedding(w, h, embed_dim=256, temperature=10000.):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        grid_w = torch.arange(int(w), dtype=torch.float32)\n",
    "        grid_h = torch.arange(int(h), dtype=torch.float32)\n",
    "        grid_w, grid_h = torch.meshgrid(grid_w, grid_h, indexing='ij')\n",
    "        assert embed_dim % 4 == 0, \\\n",
    "            'Embed dimension must be divisible by 4 for 2D sin-cos position embedding'\n",
    "        pos_dim = embed_dim // 4\n",
    "        omega = torch.arange(pos_dim, dtype=torch.float32) / pos_dim\n",
    "        omega = 1. / (temperature ** omega)\n",
    "\n",
    "        out_w = grid_w.flatten()[..., None] @ omega[None]\n",
    "        out_h = grid_h.flatten()[..., None] @ omega[None]\n",
    "\n",
    "        return torch.concat([out_w.sin(), out_w.cos(), out_h.sin(), out_h.cos()], dim=1)[None, :, :]\n",
    "\n",
    "    def forward(self, feats):\n",
    "        assert len(feats) == len(self.in_channels)\n",
    "        proj_feats = [self.input_proj[i](feat) for i, feat in enumerate(feats)]\n",
    "\n",
    "        # encoder\n",
    "        if self.num_encoder_layers > 0:\n",
    "            for i, enc_ind in enumerate(self.use_encoder_idx):\n",
    "                h, w = proj_feats[enc_ind].shape[2:]\n",
    "                # flatten [B, C, H, W] to [B, HxW, C]\n",
    "                src_flatten = proj_feats[enc_ind].flatten(2).permute(0, 2, 1)\n",
    "                if self.training or self.eval_spatial_size is None:\n",
    "                    pos_embed = self.build_2d_sincos_position_embedding(\n",
    "                        w, h, self.hidden_dim, self.pe_temperature).to(src_flatten.device)\n",
    "                else:\n",
    "                    pos_embed = getattr(self, f'pos_embed{enc_ind}', None).to(src_flatten.device)\n",
    "\n",
    "                memory :torch.Tensor = self.encoder[i](src_flatten, pos_embed=pos_embed)\n",
    "                proj_feats[enc_ind] = memory.permute(0, 2, 1).reshape(-1, self.hidden_dim, h, w).contiguous()\n",
    "\n",
    "        # broadcasting and fusion\n",
    "        inner_outs = [proj_feats[-1]]\n",
    "        for idx in range(len(self.in_channels) - 1, 0, -1):\n",
    "            feat_heigh = inner_outs[0]\n",
    "            feat_low = proj_feats[idx - 1]\n",
    "            feat_heigh = self.lateral_convs[len(self.in_channels) - 1 - idx](feat_heigh)\n",
    "            inner_outs[0] = feat_heigh\n",
    "            upsample_feat = F.interpolate(feat_heigh, scale_factor=2., mode='nearest')\n",
    "            inner_out = self.fpn_blocks[len(self.in_channels)-1-idx](torch.concat([upsample_feat, feat_low], dim=1))\n",
    "            inner_outs.insert(0, inner_out)\n",
    "\n",
    "        outs = [inner_outs[0]]\n",
    "        for idx in range(len(self.in_channels) - 1):\n",
    "            feat_low = outs[-1]\n",
    "            feat_height = inner_outs[idx + 1]\n",
    "            downsample_feat = self.downsample_convs[idx](feat_low)\n",
    "            out = self.pan_blocks[idx](torch.concat([downsample_feat, feat_height], dim=1))\n",
    "            outs.append(out)\n",
    "\n",
    "        return outs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da4bdadd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 256, 80, 80])\n",
      "torch.Size([2, 256, 40, 40])\n",
      "torch.Size([2, 256, 20, 20])\n"
     ]
    }
   ],
   "source": [
    "hybrid_encoder_instance = HybridEncoder(in_channels=[384, 768, 1536])\n",
    "encoder_outs = hybrid_encoder_instance(backbone_out)\n",
    "for i in encoder_outs:\n",
    "    print(i.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a607ff",
   "metadata": {},
   "source": [
    "## MSDeformableAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61a18164",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import functools\n",
    "from typing import List\n",
    "import torch.nn.init as init\n",
    "\n",
    "def deformable_attention_core_func_v2(\\\n",
    "    value: torch.Tensor,\n",
    "    value_spatial_shapes,\n",
    "    sampling_locations: torch.Tensor,\n",
    "    attention_weights: torch.Tensor,\n",
    "    num_points_list: List[int],\n",
    "    method='default',\n",
    "    value_shape='default',\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        value (Tensor): [bs, value_length, n_head, c]\n",
    "        value_spatial_shapes (Tensor|List): [n_levels, 2]\n",
    "        value_level_start_index (Tensor|List): [n_levels]\n",
    "        sampling_locations (Tensor): [bs, query_length, n_head, n_levels * n_points, 2]\n",
    "        attention_weights (Tensor): [bs, query_length, n_head, n_levels * n_points]\n",
    "\n",
    "    Returns:\n",
    "        output (Tensor): [bs, Length_{query}, C]\n",
    "    \"\"\"\n",
    "    # TODO find the version\n",
    "    if value_shape == 'default':\n",
    "        bs, n_head, c, _ = value[0].shape\n",
    "    elif value_shape == 'reshape':   # reshape following RT-DETR\n",
    "        bs, _, n_head, c = value.shape\n",
    "        split_shape = [h * w for h, w in value_spatial_shapes]\n",
    "        value = value.permute(0, 2, 3, 1).flatten(0, 1).split(split_shape, dim=-1)\n",
    "    _, Len_q, _, _, _ = sampling_locations.shape\n",
    "\n",
    "    # sampling_offsets [8, 480, 8, 12, 2]\n",
    "    if method == 'default':\n",
    "        sampling_grids = 2 * sampling_locations - 1\n",
    "\n",
    "    elif method == 'discrete':\n",
    "        sampling_grids = sampling_locations\n",
    "\n",
    "    sampling_grids = sampling_grids.permute(0, 2, 1, 3, 4).flatten(0, 1)\n",
    "    sampling_locations_list = sampling_grids.split(num_points_list, dim=-2)\n",
    "\n",
    "    sampling_value_list = []\n",
    "    for level, (h, w) in enumerate(value_spatial_shapes):\n",
    "        value_l = value[level].reshape(bs * n_head, c, h, w)\n",
    "        sampling_grid_l: torch.Tensor = sampling_locations_list[level]\n",
    "\n",
    "        if method == 'default':\n",
    "            sampling_value_l = F.grid_sample(\n",
    "                value_l,\n",
    "                sampling_grid_l,\n",
    "                mode='bilinear',\n",
    "                padding_mode='zeros',\n",
    "                align_corners=False)\n",
    "\n",
    "        elif method == 'discrete':\n",
    "            # n * m, seq, n, 2\n",
    "            sampling_coord = (sampling_grid_l * torch.tensor([[w, h]], device=value_l.device) + 0.5).to(torch.int64)\n",
    "\n",
    "            # FIX ME? for rectangle input\n",
    "            sampling_coord = sampling_coord.clamp(0, h - 1)\n",
    "            sampling_coord = sampling_coord.reshape(bs * n_head, Len_q * num_points_list[level], 2)\n",
    "\n",
    "            s_idx = torch.arange(sampling_coord.shape[0], device=value_l.device).unsqueeze(-1).repeat(1, sampling_coord.shape[1])\n",
    "            sampling_value_l: torch.Tensor = value_l[s_idx, :, sampling_coord[..., 1], sampling_coord[..., 0]] # n l c\n",
    "\n",
    "            sampling_value_l = sampling_value_l.permute(0, 2, 1).reshape(bs * n_head, c, Len_q, num_points_list[level])\n",
    "\n",
    "        sampling_value_list.append(sampling_value_l)\n",
    "\n",
    "    attn_weights = attention_weights.permute(0, 2, 1, 3).reshape(bs * n_head, 1, Len_q, sum(num_points_list))\n",
    "    weighted_sample_locs = torch.concat(sampling_value_list, dim=-1) * attn_weights\n",
    "    output = weighted_sample_locs.sum(-1).reshape(bs, n_head * c, Len_q)\n",
    "\n",
    "    return output.permute(0, 2, 1)\n",
    "\n",
    "class MSDeformableAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim=256,\n",
    "        num_heads=8,\n",
    "        num_levels=4,\n",
    "        num_points=4,\n",
    "        method='default',\n",
    "        offset_scale=0.5,\n",
    "    ):\n",
    "        \"\"\"Multi-Scale Deformable Attention\n",
    "        \"\"\"\n",
    "        super(MSDeformableAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.num_levels = num_levels\n",
    "        self.offset_scale = offset_scale\n",
    "\n",
    "        if isinstance(num_points, list):\n",
    "            assert len(num_points) == num_levels, ''\n",
    "            num_points_list = num_points\n",
    "        else:\n",
    "            num_points_list = [num_points for _ in range(num_levels)]\n",
    "\n",
    "        self.num_points_list = num_points_list\n",
    "\n",
    "        num_points_scale = [1/n for n in num_points_list for _ in range(n)]\n",
    "        self.register_buffer('num_points_scale', torch.tensor(num_points_scale, dtype=torch.float32))\n",
    "\n",
    "        self.total_points = num_heads * sum(num_points_list)\n",
    "        self.method = method\n",
    "\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        assert self.head_dim * num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n",
    "\n",
    "        self.sampling_offsets = nn.Linear(embed_dim, self.total_points * 2)\n",
    "        self.attention_weights = nn.Linear(embed_dim, self.total_points)\n",
    "\n",
    "        self.ms_deformable_attn_core = functools.partial(deformable_attention_core_func_v2, method=self.method)\n",
    "\n",
    "        self._reset_parameters()\n",
    "\n",
    "        if method == 'discrete':\n",
    "            for p in self.sampling_offsets.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        # sampling_offsets\n",
    "        init.constant_(self.sampling_offsets.weight, 0)\n",
    "        thetas = torch.arange(self.num_heads, dtype=torch.float32) * (2.0 * math.pi / self.num_heads)\n",
    "        grid_init = torch.stack([thetas.cos(), thetas.sin()], -1)\n",
    "        grid_init = grid_init / grid_init.abs().max(-1, keepdim=True).values\n",
    "        grid_init = grid_init.reshape(self.num_heads, 1, 2).tile([1, sum(self.num_points_list), 1])\n",
    "        scaling = torch.concat([torch.arange(1, n + 1) for n in self.num_points_list]).reshape(1, -1, 1)\n",
    "        grid_init *= scaling\n",
    "        self.sampling_offsets.bias.data[...] = grid_init.flatten()\n",
    "\n",
    "        # attention_weights\n",
    "        init.constant_(self.attention_weights.weight, 0)\n",
    "        init.constant_(self.attention_weights.bias, 0)\n",
    "\n",
    "\n",
    "    def forward(self,\n",
    "                query: torch.Tensor,\n",
    "                reference_points: torch.Tensor,\n",
    "                value: torch.Tensor,\n",
    "                value_spatial_shapes: List[int]):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            query (Tensor): [bs, query_length, C]\n",
    "            reference_points (Tensor): [bs, query_length, n_levels, 2], range in [0, 1], top-left (0,0),\n",
    "                bottom-right (1, 1), including padding area\n",
    "            value (Tensor): [bs, value_length, C]\n",
    "            value_spatial_shapes (List): [n_levels, 2], [(H_0, W_0), (H_1, W_1), ..., (H_{L-1}, W_{L-1})]\n",
    "\n",
    "        Returns:\n",
    "            output (Tensor): [bs, Length_{query}, C]\n",
    "        \"\"\"\n",
    "        bs, Len_q = query.shape[:2]\n",
    "\n",
    "        sampling_offsets: torch.Tensor = self.sampling_offsets(query)\n",
    "        sampling_offsets = sampling_offsets.reshape(bs, Len_q, self.num_heads, sum(self.num_points_list), 2)\n",
    "\n",
    "        attention_weights = self.attention_weights(query).reshape(bs, Len_q, self.num_heads, sum(self.num_points_list))\n",
    "        attention_weights = F.softmax(attention_weights, dim=-1)\n",
    "\n",
    "        if reference_points.shape[-1] == 2:\n",
    "            offset_normalizer = torch.tensor(value_spatial_shapes)\n",
    "            offset_normalizer = offset_normalizer.flip([1]).reshape(1, 1, 1, self.num_levels, 1, 2)\n",
    "            sampling_locations = reference_points.reshape(bs, Len_q, 1, self.num_levels, 1, 2) + sampling_offsets / offset_normalizer\n",
    "        elif reference_points.shape[-1] == 4:\n",
    "            # reference_points [8, 480, None, 1,  4]\n",
    "            # sampling_offsets [8, 480, 8,    12, 2]\n",
    "            num_points_scale = self.num_points_scale.to(dtype=query.dtype).unsqueeze(-1)\n",
    "            offset = sampling_offsets * num_points_scale * reference_points[:, :, None, :, 2:] * self.offset_scale\n",
    "            sampling_locations = reference_points[:, :, None, :, :2] + offset\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Last dim of reference_points must be 2 or 4, but get {} instead.\".\n",
    "                format(reference_points.shape[-1]))\n",
    "\n",
    "        output = self.ms_deformable_attn_core(value, value_spatial_shapes, sampling_locations, attention_weights, self.num_points_list)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "66cf51a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before cross attention: torch.Size([4, 300, 256])\n",
      "after cross attention: torch.Size([4, 300, 256])\n"
     ]
    }
   ],
   "source": [
    "self_attn = nn.MultiheadAttention(embed_dim=256, num_heads=8, dropout=0.0, batch_first=True)\n",
    "cross_attn = MSDeformableAttention(embed_dim=256, num_heads=8, num_levels=3, num_points=4, method=\"default\")\n",
    "\n",
    "q = k = torch.rand(size=(4, 300, 256))\n",
    "target = torch.rand(size=(4, 300, 256))\n",
    "\n",
    "target2, _ = self_attn(q, k, value=target, attn_mask=None)\n",
    "print(f'before cross attention: {target2.shape}')\n",
    "\n",
    "spatial_shapes = [[80, 80], [40, 40], [20, 20]]\n",
    "v0 = torch.rand(size=(4, 8, 32, 6400))\n",
    "v1 = torch.rand(size=(4, 8, 32, 1600))\n",
    "v2 = torch.rand(size=(4, 8, 32, 400))\n",
    "value = [v0, v1, v2]\n",
    "\n",
    "reference_points = torch.rand(size=(4, 300, 1, 4))\n",
    "\n",
    "target2 = cross_attn(\n",
    "    target,\n",
    "    reference_points,\n",
    "    value,\n",
    "    spatial_shapes\n",
    ")\n",
    "\n",
    "print(f'after cross attention: {target2.shape}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
