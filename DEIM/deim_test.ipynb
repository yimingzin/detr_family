{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97d38382",
   "metadata": {},
   "source": [
    "# backbone: PResNet or HGnetv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10d5e4e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 384, 80, 80])\n",
      "torch.Size([2, 768, 40, 40])\n",
      "torch.Size([2, 1536, 20, 20])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "S3 = torch.rand(size=(2, 384, 80, 80))\n",
    "S4 = torch.rand(size=(2, 768, 40, 40))\n",
    "S5 = torch.rand(size=(2, 1536, 20, 20))\n",
    "\n",
    "backbone_out = [S3, S4, S5]\n",
    "\n",
    "for i in backbone_out:\n",
    "    print(i.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1d861a",
   "metadata": {},
   "source": [
    "# Hybrid Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e863ca28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activation(act: str, inpace: bool=True):\n",
    "    \"\"\"get activation\n",
    "    \"\"\"\n",
    "    if act is None:\n",
    "        return nn.Identity()\n",
    "\n",
    "    elif isinstance(act, nn.Module):\n",
    "        return act\n",
    "\n",
    "    act = act.lower()\n",
    "\n",
    "    if act == 'silu' or act == 'swish':\n",
    "        m = nn.SiLU()\n",
    "\n",
    "    elif act == 'relu':\n",
    "        m = nn.ReLU()\n",
    "\n",
    "    elif act == 'leaky_relu':\n",
    "        m = nn.LeakyReLU()\n",
    "\n",
    "    elif act == 'silu':\n",
    "        m = nn.SiLU()\n",
    "\n",
    "    elif act == 'gelu':\n",
    "        m = nn.GELU()\n",
    "\n",
    "    elif act == 'hardsigmoid':\n",
    "        m = nn.Hardsigmoid()\n",
    "\n",
    "    else:\n",
    "        raise RuntimeError('')\n",
    "\n",
    "    if hasattr(m, 'inplace'):\n",
    "        m.inplace = inpace\n",
    "\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3598444",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNormLayer_fuse(nn.Module):\n",
    "    def __init__(self, ch_in, ch_out, kernel_size, stride, g=1, padding=None, bias=False, act=None):\n",
    "        super().__init__()\n",
    "        padding = (kernel_size-1)//2 if padding is None else padding\n",
    "        self.conv = nn.Conv2d(\n",
    "            ch_in,\n",
    "            ch_out,\n",
    "            kernel_size,\n",
    "            stride,\n",
    "            groups=g,\n",
    "            padding=padding,\n",
    "            bias=bias)\n",
    "        self.norm = nn.BatchNorm2d(ch_out)\n",
    "        self.act = nn.Identity() if act is None else get_activation(act)\n",
    "        self.ch_in, self.ch_out, self.kernel_size, self.stride, self.g, self.padding, self.bias = \\\n",
    "            ch_in, ch_out, kernel_size, stride, g, padding, bias\n",
    "\n",
    "    def forward(self, x):\n",
    "        if hasattr(self, 'conv_bn_fused'):\n",
    "            y = self.conv_bn_fused(x)\n",
    "        else:\n",
    "            y = self.norm(self.conv(x))\n",
    "        return self.act(y)\n",
    "\n",
    "    def convert_to_deploy(self):\n",
    "        if not hasattr(self, 'conv_bn_fused'):\n",
    "            self.conv_bn_fused = nn.Conv2d(\n",
    "                self.ch_in,\n",
    "                self.ch_out,\n",
    "                self.kernel_size,\n",
    "                self.stride,\n",
    "                groups=self.g,\n",
    "                padding=self.padding,\n",
    "                bias=True)\n",
    "\n",
    "        kernel, bias = self.get_equivalent_kernel_bias()\n",
    "        self.conv_bn_fused.weight.data = kernel\n",
    "        self.conv_bn_fused.bias.data = bias\n",
    "        self.__delattr__('conv')\n",
    "        self.__delattr__('norm')\n",
    "\n",
    "    def get_equivalent_kernel_bias(self):\n",
    "        kernel3x3, bias3x3 = self._fuse_bn_tensor()\n",
    "\n",
    "        return kernel3x3, bias3x3\n",
    "\n",
    "    def _fuse_bn_tensor(self):\n",
    "        kernel = self.conv.weight\n",
    "        running_mean = self.norm.running_mean\n",
    "        running_var = self.norm.running_var\n",
    "        gamma = self.norm.weight\n",
    "        beta = self.norm.bias\n",
    "        eps = self.norm.eps\n",
    "        std = (running_var + eps).sqrt()\n",
    "        t = (gamma / std).reshape(-1, 1, 1, 1)\n",
    "        return kernel * t, beta - running_mean * gamma / std\n",
    "\n",
    "\n",
    "class ConvNormLayer(nn.Module):\n",
    "    def __init__(self, ch_in, ch_out, kernel_size, stride, g=1, padding=None, bias=False, act=None):\n",
    "        super().__init__()\n",
    "        padding = (kernel_size-1)//2 if padding is None else padding\n",
    "        self.conv = nn.Conv2d(\n",
    "            ch_in,\n",
    "            ch_out,\n",
    "            kernel_size,\n",
    "            stride,\n",
    "            groups=g,\n",
    "            padding=padding,\n",
    "            bias=bias)\n",
    "        self.norm = nn.BatchNorm2d(ch_out)\n",
    "        self.act = nn.Identity() if act is None else get_activation(act)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.act(self.norm(self.conv(x)))\n",
    "    \n",
    "# TODO, add activation for cv1 following YOLOv10\n",
    "# self.cv1 = Conv(c1, c2, 1, 1)\n",
    "# self.cv2 = Conv(c2, c2, k=k, s=s, g=c2, act=False)\n",
    "class SCDown(nn.Module):\n",
    "    def __init__(self, c1, c2, k, s, act=None):\n",
    "        super().__init__()\n",
    "        self.cv1 = ConvNormLayer_fuse(c1, c2, 1, 1)\n",
    "        self.cv2 = ConvNormLayer_fuse(c2, c2, k, s, c2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.cv2(self.cv1(x))\n",
    "\n",
    "\n",
    "class VGGBlock(nn.Module):\n",
    "    def __init__(self, ch_in, ch_out, act='relu'):\n",
    "        super().__init__()\n",
    "        self.ch_in = ch_in\n",
    "        self.ch_out = ch_out\n",
    "        self.conv1 = ConvNormLayer(ch_in, ch_out, 3, 1, padding=1, act=None)\n",
    "        self.conv2 = ConvNormLayer(ch_in, ch_out, 1, 1, padding=0, act=None)\n",
    "        self.act = nn.Identity() if act is None else get_activation(act)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if hasattr(self, 'conv'):\n",
    "            y = self.conv(x)\n",
    "        else:\n",
    "            y = self.conv1(x) + self.conv2(x)\n",
    "\n",
    "        return self.act(y)\n",
    "\n",
    "    def convert_to_deploy(self):\n",
    "        if not hasattr(self, 'conv'):\n",
    "            self.conv = nn.Conv2d(self.ch_in, self.ch_out, 3, 1, padding=1)\n",
    "\n",
    "        kernel, bias = self.get_equivalent_kernel_bias()\n",
    "        self.conv.weight.data = kernel\n",
    "        self.conv.bias.data = bias\n",
    "        self.__delattr__('conv1')\n",
    "        self.__delattr__('conv2')\n",
    "\n",
    "    def get_equivalent_kernel_bias(self):\n",
    "        kernel3x3, bias3x3 = self._fuse_bn_tensor(self.conv1)\n",
    "        kernel1x1, bias1x1 = self._fuse_bn_tensor(self.conv2)\n",
    "\n",
    "        return kernel3x3 + self._pad_1x1_to_3x3_tensor(kernel1x1), bias3x3 + bias1x1\n",
    "\n",
    "    def _pad_1x1_to_3x3_tensor(self, kernel1x1):\n",
    "        if kernel1x1 is None:\n",
    "            return 0\n",
    "        else:\n",
    "            return F.pad(kernel1x1, [1, 1, 1, 1])\n",
    "\n",
    "    def _fuse_bn_tensor(self, branch: ConvNormLayer):\n",
    "        if branch is None:\n",
    "            return 0, 0\n",
    "        kernel = branch.conv.weight\n",
    "        running_mean = branch.norm.running_mean\n",
    "        running_var = branch.norm.running_var\n",
    "        gamma = branch.norm.weight\n",
    "        beta = branch.norm.bias\n",
    "        eps = branch.norm.eps\n",
    "        std = (running_var + eps).sqrt()\n",
    "        t = (gamma / std).reshape(-1, 1, 1, 1)\n",
    "        return kernel * t, beta - running_mean * gamma / std\n",
    "\n",
    "\n",
    "class CSPLayer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 num_blocks=3,\n",
    "                 expansion=1.0,\n",
    "                 bias=False,\n",
    "                 act=\"silu\",\n",
    "                 bottletype=VGGBlock):\n",
    "        super(CSPLayer, self).__init__()\n",
    "        hidden_channels = int(out_channels * expansion)\n",
    "        self.conv1 = ConvNormLayer_fuse(in_channels, hidden_channels, 1, 1, bias=bias, act=act)\n",
    "        self.conv2 = ConvNormLayer_fuse(in_channels, hidden_channels, 1, 1, bias=bias, act=act)\n",
    "        self.bottlenecks = nn.Sequential(*[\n",
    "            bottletype(hidden_channels, hidden_channels, act=act) for _ in range(num_blocks)\n",
    "        ])\n",
    "        if hidden_channels != out_channels:\n",
    "            self.conv3 = ConvNormLayer_fuse(hidden_channels, out_channels, 1, 1, bias=bias, act=act)\n",
    "        else:\n",
    "            self.conv3 = nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_2 = self.conv2(x)\n",
    "        x_1 = self.conv1(x)\n",
    "        x_1 = self.bottlenecks(x_1)\n",
    "        return self.conv3(x_1 + x_2)\n",
    "\n",
    "class RepNCSPELAN4(nn.Module):\n",
    "    # csp-elan\n",
    "    def __init__(self, c1, c2, c3, c4, n=3,\n",
    "                 bias=False,\n",
    "                 act=\"silu\"):\n",
    "        super().__init__()\n",
    "        self.c = c3//2\n",
    "        self.cv1 = ConvNormLayer_fuse(c1, c3, 1, 1, bias=bias, act=act)\n",
    "        self.cv2 = nn.Sequential(CSPLayer(c3//2, c4, n, 1, bias=bias, act=act, bottletype=VGGBlock), ConvNormLayer_fuse(c4, c4, 3, 1, bias=bias, act=act))\n",
    "        self.cv3 = nn.Sequential(CSPLayer(c4, c4, n, 1, bias=bias, act=act, bottletype=VGGBlock), ConvNormLayer_fuse(c4, c4, 3, 1, bias=bias, act=act))\n",
    "        self.cv4 = ConvNormLayer_fuse(c3+(2*c4), c2, 1, 1, bias=bias, act=act)\n",
    "\n",
    "    def forward_chunk(self, x):\n",
    "        y = list(self.cv1(x).chunk(2, 1))\n",
    "        y.extend((m(y[-1])) for m in [self.cv2, self.cv3])\n",
    "        return self.cv4(torch.cat(y, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = list(self.cv1(x).split((self.c, self.c), 1))\n",
    "        y.extend(m(y[-1]) for m in [self.cv2, self.cv3])\n",
    "        return self.cv4(torch.cat(y, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a1dc96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "# transformer\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 d_model,\n",
    "                 nhead,\n",
    "                 dim_feedforward=2048,\n",
    "                 dropout=0.1,\n",
    "                 activation=\"relu\",\n",
    "                 normalize_before=False):\n",
    "        super().__init__()\n",
    "        self.normalize_before = normalize_before\n",
    "\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout, batch_first=True)\n",
    "\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.activation = get_activation(activation)\n",
    "\n",
    "    @staticmethod\n",
    "    def with_pos_embed(tensor, pos_embed):\n",
    "        return tensor if pos_embed is None else tensor + pos_embed\n",
    "\n",
    "    def forward(self, src, src_mask=None, pos_embed=None) -> torch.Tensor:\n",
    "        residual = src\n",
    "        if self.normalize_before:\n",
    "            src = self.norm1(src)\n",
    "        q = k = self.with_pos_embed(src, pos_embed)\n",
    "        src, _ = self.self_attn(q, k, value=src, attn_mask=src_mask)\n",
    "\n",
    "        src = residual + self.dropout1(src)\n",
    "        if not self.normalize_before:\n",
    "            src = self.norm1(src)\n",
    "\n",
    "        residual = src\n",
    "        if self.normalize_before:\n",
    "            src = self.norm2(src)\n",
    "        src = self.linear2(self.dropout(self.activation(self.linear1(src))))\n",
    "        src = residual + self.dropout2(src)\n",
    "        if not self.normalize_before:\n",
    "            src = self.norm2(src)\n",
    "        return src\n",
    "\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, encoder_layer, num_layers, norm=None):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.layers = nn.ModuleList([copy.deepcopy(encoder_layer) for _ in range(num_layers)])\n",
    "        self.num_layers = num_layers\n",
    "        self.norm = norm\n",
    "\n",
    "    def forward(self, src, src_mask=None, pos_embed=None) -> torch.Tensor:\n",
    "        output = src\n",
    "        for layer in self.layers:\n",
    "            output = layer(output, src_mask=src_mask, pos_embed=pos_embed)\n",
    "\n",
    "        if self.norm is not None:\n",
    "            output = self.norm(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9084868",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class HybridEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels=[512, 1024, 2048],\n",
    "                 feat_strides=[8, 16, 32],\n",
    "                 hidden_dim=256,\n",
    "                 nhead=8,\n",
    "                 dim_feedforward = 1024,\n",
    "                 dropout=0.0,\n",
    "                 enc_act='gelu',\n",
    "                 use_encoder_idx=[2],\n",
    "                 num_encoder_layers=1,\n",
    "                 pe_temperature=10000,\n",
    "                 expansion=1.0,\n",
    "                 depth_mult=1.0,\n",
    "                 act='silu',\n",
    "                 eval_spatial_size=None,\n",
    "                 version='dfine',\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.feat_strides = feat_strides\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.use_encoder_idx = use_encoder_idx\n",
    "        self.num_encoder_layers = num_encoder_layers\n",
    "        self.pe_temperature = pe_temperature\n",
    "        self.eval_spatial_size = eval_spatial_size\n",
    "        self.out_channels = [hidden_dim for _ in range(len(in_channels))]\n",
    "        self.out_strides = feat_strides\n",
    "\n",
    "        # channel projection\n",
    "        self.input_proj = nn.ModuleList()\n",
    "        for in_channel in in_channels:\n",
    "            proj = nn.Sequential(OrderedDict([\n",
    "                    ('conv', nn.Conv2d(in_channel, hidden_dim, kernel_size=1, bias=False)),\n",
    "                    ('norm', nn.BatchNorm2d(hidden_dim))\n",
    "                ]))\n",
    "\n",
    "            self.input_proj.append(proj)\n",
    "\n",
    "        # encoder transformer\n",
    "        encoder_layer = TransformerEncoderLayer(\n",
    "            hidden_dim,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            activation=enc_act\n",
    "            )\n",
    "\n",
    "        self.encoder = nn.ModuleList([\n",
    "            TransformerEncoder(copy.deepcopy(encoder_layer), num_encoder_layers) for _ in range(len(use_encoder_idx))\n",
    "        ])\n",
    "\n",
    "        # top-down fpn\n",
    "        self.lateral_convs = nn.ModuleList()\n",
    "        self.fpn_blocks = nn.ModuleList()\n",
    "        for _ in range(len(in_channels) - 1, 0, -1):\n",
    "            # TODO, add activation for those lateral convs\n",
    "            if version == 'dfine':\n",
    "                self.lateral_convs.append(ConvNormLayer_fuse(hidden_dim, hidden_dim, 1, 1))\n",
    "            else:\n",
    "                self.lateral_convs.append(ConvNormLayer_fuse(hidden_dim, hidden_dim, 1, 1, act=act))\n",
    "            self.fpn_blocks.append(\n",
    "                RepNCSPELAN4(hidden_dim * 2, hidden_dim, hidden_dim * 2, round(expansion * hidden_dim // 2), round(3 * depth_mult), act=act) \\\n",
    "                if version == 'dfine' else CSPLayer(hidden_dim * 2, hidden_dim, round(3 * depth_mult), act=act, expansion=expansion, bottletype=VGGBlock)\n",
    "            )\n",
    "\n",
    "        # bottom-up pan\n",
    "        self.downsample_convs = nn.ModuleList()\n",
    "        self.pan_blocks = nn.ModuleList()\n",
    "        for _ in range(len(in_channels) - 1):\n",
    "            self.downsample_convs.append(\n",
    "                nn.Sequential(SCDown(hidden_dim, hidden_dim, 3, 2, act=act)) \\\n",
    "                if version == 'dfine' else ConvNormLayer_fuse(hidden_dim, hidden_dim, 3, 2, act=act)\n",
    "            )\n",
    "            self.pan_blocks.append(\n",
    "                RepNCSPELAN4(hidden_dim * 2, hidden_dim, hidden_dim * 2, round(expansion * hidden_dim // 2), round(3 * depth_mult), act=act) \\\n",
    "                if version == 'dfine' else CSPLayer(hidden_dim * 2, hidden_dim, round(3 * depth_mult), act=act, expansion=expansion, bottletype=VGGBlock)\n",
    "            )\n",
    "\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        if self.eval_spatial_size:\n",
    "            for idx in self.use_encoder_idx:\n",
    "                stride = self.feat_strides[idx]\n",
    "                pos_embed = self.build_2d_sincos_position_embedding(\n",
    "                    self.eval_spatial_size[1] // stride, self.eval_spatial_size[0] // stride,\n",
    "                    self.hidden_dim, self.pe_temperature)\n",
    "                setattr(self, f'pos_embed{idx}', pos_embed)\n",
    "                # self.register_buffer(f'pos_embed{idx}', pos_embed)\n",
    "\n",
    "    @staticmethod\n",
    "    def build_2d_sincos_position_embedding(w, h, embed_dim=256, temperature=10000.):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        grid_w = torch.arange(int(w), dtype=torch.float32)\n",
    "        grid_h = torch.arange(int(h), dtype=torch.float32)\n",
    "        grid_w, grid_h = torch.meshgrid(grid_w, grid_h, indexing='ij')\n",
    "        assert embed_dim % 4 == 0, \\\n",
    "            'Embed dimension must be divisible by 4 for 2D sin-cos position embedding'\n",
    "        pos_dim = embed_dim // 4\n",
    "        omega = torch.arange(pos_dim, dtype=torch.float32) / pos_dim\n",
    "        omega = 1. / (temperature ** omega)\n",
    "\n",
    "        out_w = grid_w.flatten()[..., None] @ omega[None]\n",
    "        out_h = grid_h.flatten()[..., None] @ omega[None]\n",
    "\n",
    "        return torch.concat([out_w.sin(), out_w.cos(), out_h.sin(), out_h.cos()], dim=1)[None, :, :]\n",
    "\n",
    "    def forward(self, feats):\n",
    "        assert len(feats) == len(self.in_channels)\n",
    "        proj_feats = [self.input_proj[i](feat) for i, feat in enumerate(feats)]\n",
    "\n",
    "        # encoder\n",
    "        if self.num_encoder_layers > 0:\n",
    "            for i, enc_ind in enumerate(self.use_encoder_idx):\n",
    "                h, w = proj_feats[enc_ind].shape[2:]\n",
    "                # flatten [B, C, H, W] to [B, HxW, C]\n",
    "                src_flatten = proj_feats[enc_ind].flatten(2).permute(0, 2, 1)\n",
    "                if self.training or self.eval_spatial_size is None:\n",
    "                    pos_embed = self.build_2d_sincos_position_embedding(\n",
    "                        w, h, self.hidden_dim, self.pe_temperature).to(src_flatten.device)\n",
    "                else:\n",
    "                    pos_embed = getattr(self, f'pos_embed{enc_ind}', None).to(src_flatten.device)\n",
    "\n",
    "                memory :torch.Tensor = self.encoder[i](src_flatten, pos_embed=pos_embed)\n",
    "                proj_feats[enc_ind] = memory.permute(0, 2, 1).reshape(-1, self.hidden_dim, h, w).contiguous()\n",
    "\n",
    "        # broadcasting and fusion\n",
    "        inner_outs = [proj_feats[-1]]\n",
    "        for idx in range(len(self.in_channels) - 1, 0, -1):\n",
    "            feat_heigh = inner_outs[0]\n",
    "            feat_low = proj_feats[idx - 1]\n",
    "            feat_heigh = self.lateral_convs[len(self.in_channels) - 1 - idx](feat_heigh)\n",
    "            inner_outs[0] = feat_heigh\n",
    "            upsample_feat = F.interpolate(feat_heigh, scale_factor=2., mode='nearest')\n",
    "            inner_out = self.fpn_blocks[len(self.in_channels)-1-idx](torch.concat([upsample_feat, feat_low], dim=1))\n",
    "            inner_outs.insert(0, inner_out)\n",
    "\n",
    "        outs = [inner_outs[0]]\n",
    "        for idx in range(len(self.in_channels) - 1):\n",
    "            feat_low = outs[-1]\n",
    "            feat_height = inner_outs[idx + 1]\n",
    "            downsample_feat = self.downsample_convs[idx](feat_low)\n",
    "            out = self.pan_blocks[idx](torch.concat([downsample_feat, feat_height], dim=1))\n",
    "            outs.append(out)\n",
    "\n",
    "        return outs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da4bdadd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 256, 80, 80])\n",
      "torch.Size([2, 256, 40, 40])\n",
      "torch.Size([2, 256, 20, 20])\n"
     ]
    }
   ],
   "source": [
    "hybrid_encoder_instance = HybridEncoder(in_channels=[384, 768, 1536])\n",
    "encoder_outs = hybrid_encoder_instance(backbone_out)\n",
    "for i in encoder_outs:\n",
    "    print(i.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a607ff",
   "metadata": {},
   "source": [
    "## MSDeformableAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61a18164",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import functools\n",
    "from typing import List\n",
    "import torch.nn.init as init\n",
    "\n",
    "def deformable_attention_core_func_v2(\\\n",
    "    value: torch.Tensor,\n",
    "    value_spatial_shapes,\n",
    "    sampling_locations: torch.Tensor,\n",
    "    attention_weights: torch.Tensor,\n",
    "    num_points_list: List[int],\n",
    "    method='default',\n",
    "    value_shape='default',\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        value (Tensor): [bs, value_length, n_head, c]\n",
    "        value_spatial_shapes (Tensor|List): [n_levels, 2]\n",
    "        value_level_start_index (Tensor|List): [n_levels]\n",
    "        sampling_locations (Tensor): [bs, query_length, n_head, n_levels * n_points, 2]\n",
    "        attention_weights (Tensor): [bs, query_length, n_head, n_levels * n_points]\n",
    "\n",
    "    Returns:\n",
    "        output (Tensor): [bs, Length_{query}, C]\n",
    "    \"\"\"\n",
    "    # TODO find the version\n",
    "    if value_shape == 'default':\n",
    "        bs, n_head, c, _ = value[0].shape\n",
    "    elif value_shape == 'reshape':   # reshape following RT-DETR\n",
    "        bs, _, n_head, c = value.shape\n",
    "        split_shape = [h * w for h, w in value_spatial_shapes]\n",
    "        value = value.permute(0, 2, 3, 1).flatten(0, 1).split(split_shape, dim=-1)\n",
    "    _, Len_q, _, _, _ = sampling_locations.shape\n",
    "\n",
    "    # sampling_offsets [8, 480, 8, 12, 2]\n",
    "    if method == 'default':\n",
    "        sampling_grids = 2 * sampling_locations - 1\n",
    "\n",
    "    elif method == 'discrete':\n",
    "        sampling_grids = sampling_locations\n",
    "\n",
    "    sampling_grids = sampling_grids.permute(0, 2, 1, 3, 4).flatten(0, 1)\n",
    "    sampling_locations_list = sampling_grids.split(num_points_list, dim=-2)\n",
    "\n",
    "    sampling_value_list = []\n",
    "    for level, (h, w) in enumerate(value_spatial_shapes):\n",
    "        value_l = value[level].reshape(bs * n_head, c, h, w)\n",
    "        sampling_grid_l: torch.Tensor = sampling_locations_list[level]\n",
    "\n",
    "        if method == 'default':\n",
    "            sampling_value_l = F.grid_sample(\n",
    "                value_l,\n",
    "                sampling_grid_l,\n",
    "                mode='bilinear',\n",
    "                padding_mode='zeros',\n",
    "                align_corners=False)\n",
    "\n",
    "        elif method == 'discrete':\n",
    "            # n * m, seq, n, 2\n",
    "            sampling_coord = (sampling_grid_l * torch.tensor([[w, h]], device=value_l.device) + 0.5).to(torch.int64)\n",
    "\n",
    "            # FIX ME? for rectangle input\n",
    "            sampling_coord = sampling_coord.clamp(0, h - 1)\n",
    "            sampling_coord = sampling_coord.reshape(bs * n_head, Len_q * num_points_list[level], 2)\n",
    "\n",
    "            s_idx = torch.arange(sampling_coord.shape[0], device=value_l.device).unsqueeze(-1).repeat(1, sampling_coord.shape[1])\n",
    "            sampling_value_l: torch.Tensor = value_l[s_idx, :, sampling_coord[..., 1], sampling_coord[..., 0]] # n l c\n",
    "\n",
    "            sampling_value_l = sampling_value_l.permute(0, 2, 1).reshape(bs * n_head, c, Len_q, num_points_list[level])\n",
    "\n",
    "        sampling_value_list.append(sampling_value_l)\n",
    "\n",
    "    attn_weights = attention_weights.permute(0, 2, 1, 3).reshape(bs * n_head, 1, Len_q, sum(num_points_list))\n",
    "    weighted_sample_locs = torch.concat(sampling_value_list, dim=-1) * attn_weights\n",
    "    output = weighted_sample_locs.sum(-1).reshape(bs, n_head * c, Len_q)\n",
    "\n",
    "    return output.permute(0, 2, 1)\n",
    "\n",
    "\n",
    "class MSDeformableAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim=256,\n",
    "        num_heads=8,\n",
    "        num_levels=4,\n",
    "        num_points=4,\n",
    "        method='default',\n",
    "        offset_scale=0.5,\n",
    "    ):\n",
    "        \"\"\"Multi-Scale Deformable Attention\n",
    "        \"\"\"\n",
    "        super(MSDeformableAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.num_levels = num_levels\n",
    "        self.offset_scale = offset_scale\n",
    "\n",
    "        if isinstance(num_points, list):\n",
    "            assert len(num_points) == num_levels, ''\n",
    "            num_points_list = num_points\n",
    "        else:\n",
    "            num_points_list = [num_points for _ in range(num_levels)]\n",
    "\n",
    "        self.num_points_list = num_points_list\n",
    "\n",
    "        num_points_scale = [1/n for n in num_points_list for _ in range(n)]\n",
    "        self.register_buffer('num_points_scale', torch.tensor(num_points_scale, dtype=torch.float32))\n",
    "\n",
    "        self.total_points = num_heads * sum(num_points_list)\n",
    "        self.method = method\n",
    "\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        assert self.head_dim * num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n",
    "\n",
    "        self.sampling_offsets = nn.Linear(embed_dim, self.total_points * 2)\n",
    "        self.attention_weights = nn.Linear(embed_dim, self.total_points)\n",
    "\n",
    "        self.ms_deformable_attn_core = functools.partial(deformable_attention_core_func_v2, method=self.method)\n",
    "\n",
    "        self._reset_parameters()\n",
    "\n",
    "        if method == 'discrete':\n",
    "            for p in self.sampling_offsets.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        # sampling_offsets\n",
    "        init.constant_(self.sampling_offsets.weight, 0)\n",
    "        thetas = torch.arange(self.num_heads, dtype=torch.float32) * (2.0 * math.pi / self.num_heads)\n",
    "        grid_init = torch.stack([thetas.cos(), thetas.sin()], -1)\n",
    "        grid_init = grid_init / grid_init.abs().max(-1, keepdim=True).values\n",
    "        grid_init = grid_init.reshape(self.num_heads, 1, 2).tile([1, sum(self.num_points_list), 1])\n",
    "        scaling = torch.concat([torch.arange(1, n + 1) for n in self.num_points_list]).reshape(1, -1, 1)\n",
    "        grid_init *= scaling\n",
    "        self.sampling_offsets.bias.data[...] = grid_init.flatten()\n",
    "\n",
    "        # attention_weights\n",
    "        init.constant_(self.attention_weights.weight, 0)\n",
    "        init.constant_(self.attention_weights.bias, 0)\n",
    "\n",
    "\n",
    "    def forward(self,\n",
    "                query: torch.Tensor,\n",
    "                reference_points: torch.Tensor,\n",
    "                value: torch.Tensor,\n",
    "                value_spatial_shapes: List[int]):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            query (Tensor): [bs, query_length, C]\n",
    "            reference_points (Tensor): [bs, query_length, n_levels, 2], range in [0, 1], top-left (0,0),\n",
    "                bottom-right (1, 1), including padding area\n",
    "            value (Tensor): [bs, value_length, C]\n",
    "            value_spatial_shapes (List): [n_levels, 2], [(H_0, W_0), (H_1, W_1), ..., (H_{L-1}, W_{L-1})]\n",
    "\n",
    "        Returns:\n",
    "            output (Tensor): [bs, Length_{query}, C]\n",
    "        \"\"\"\n",
    "        bs, Len_q = query.shape[:2]\n",
    "\n",
    "        sampling_offsets: torch.Tensor = self.sampling_offsets(query)\n",
    "        sampling_offsets = sampling_offsets.reshape(bs, Len_q, self.num_heads, sum(self.num_points_list), 2)\n",
    "\n",
    "        attention_weights = self.attention_weights(query).reshape(bs, Len_q, self.num_heads, sum(self.num_points_list))\n",
    "        attention_weights = F.softmax(attention_weights, dim=-1)\n",
    "\n",
    "        if reference_points.shape[-1] == 2:\n",
    "            offset_normalizer = torch.tensor(value_spatial_shapes)\n",
    "            offset_normalizer = offset_normalizer.flip([1]).reshape(1, 1, 1, self.num_levels, 1, 2)\n",
    "            sampling_locations = reference_points.reshape(bs, Len_q, 1, self.num_levels, 1, 2) + sampling_offsets / offset_normalizer\n",
    "        elif reference_points.shape[-1] == 4:\n",
    "            # reference_points [8, 480, None, 1,  4]\n",
    "            # sampling_offsets [8, 480, 8,    12, 2]\n",
    "            num_points_scale = self.num_points_scale.to(dtype=query.dtype).unsqueeze(-1)\n",
    "            offset = sampling_offsets * num_points_scale * reference_points[:, :, None, :, 2:] * self.offset_scale\n",
    "            sampling_locations = reference_points[:, :, None, :, :2] + offset\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Last dim of reference_points must be 2 or 4, but get {} instead.\".\n",
    "                format(reference_points.shape[-1]))\n",
    "\n",
    "        output = self.ms_deformable_attn_core(value, value_spatial_shapes, sampling_locations, attention_weights, self.num_points_list)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66cf51a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before cross attention: torch.Size([2, 300, 256])\n",
      "after cross attention: torch.Size([2, 300, 256])\n"
     ]
    }
   ],
   "source": [
    "self_attn = nn.MultiheadAttention(embed_dim=256, num_heads=8, dropout=0.0, batch_first=True)\n",
    "cross_attn = MSDeformableAttention(embed_dim=256, num_heads=8, num_levels=3, num_points=4, method=\"default\")\n",
    "\n",
    "q = k = torch.rand(size=(2, 300, 256))\n",
    "target = torch.rand(size=(2, 300, 256))\n",
    "\n",
    "target2, _ = self_attn(q, k, value=target, attn_mask=None)\n",
    "print(f'before cross attention: {target2.shape}')\n",
    "\n",
    "spatial_shapes = [[80, 80], [40, 40], [20, 20]]\n",
    "v0 = torch.rand(size=(2, 8, 32, 6400))\n",
    "v1 = torch.rand(size=(2, 8, 32, 1600))\n",
    "v2 = torch.rand(size=(2, 8, 32, 400))\n",
    "value = [v0, v1, v2]\n",
    "\n",
    "reference_points = torch.rand(size=(2, 300, 1, 4))\n",
    "\n",
    "target2 = cross_attn(\n",
    "    target,\n",
    "    reference_points,\n",
    "    value,\n",
    "    spatial_shapes\n",
    ")\n",
    "\n",
    "print(f'after cross attention: {target2.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da0edffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModuleList(\n",
      "  (0-2): 3 x Identity()\n",
      ")\n",
      "torch.Size([2, 8400, 256])\n",
      "[[80, 80], [40, 40], [20, 20]]\n"
     ]
    }
   ],
   "source": [
    "num_levels = 3\n",
    "\n",
    "def _build_input_proj_layer(feat_channels):\n",
    "        input_proj = nn.ModuleList()\n",
    "        for in_channels in feat_channels:\n",
    "            if in_channels == 256:\n",
    "                input_proj.append(nn.Identity())\n",
    "            else:\n",
    "                input_proj.append(\n",
    "                    nn.Sequential(OrderedDict([\n",
    "                        ('conv', nn.Conv2d(in_channels, 256, 1, bias=False)),\n",
    "                        ('norm', nn.BatchNorm2d(256,))])\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        in_channels = feat_channels[-1]\n",
    "\n",
    "        for _ in range(3 - len(feat_channels)):\n",
    "            if in_channels == 256:\n",
    "                input_proj.append(nn.Identity())\n",
    "            else:\n",
    "                input_proj.append(\n",
    "                    nn.Sequential(OrderedDict([\n",
    "                        ('conv', nn.Conv2d(in_channels, 256, 3, 2, padding=1, bias=False)),\n",
    "                        ('norm', nn.BatchNorm2d(256))])\n",
    "                    )\n",
    "                )\n",
    "                in_channels = 256\n",
    "                \n",
    "        return input_proj\n",
    "\n",
    "input_proj = _build_input_proj_layer([256, 256, 256])\n",
    "print(input_proj)\n",
    "\n",
    "\n",
    "def _get_encoder_input(feats: List[torch.Tensor]):\n",
    "        # get projection features\n",
    "        proj_feats = [input_proj[i](feat) for i, feat in enumerate(feats)]\n",
    "        if num_levels > len(proj_feats):\n",
    "            len_srcs = len(proj_feats)\n",
    "            for i in range(len_srcs, num_levels):\n",
    "                if i == len_srcs:\n",
    "                    proj_feats.append(input_proj[i](feats[-1]))\n",
    "                else:\n",
    "                    proj_feats.append(input_proj[i](proj_feats[-1]))\n",
    "\n",
    "        # get encoder inputs\n",
    "        feat_flatten = []\n",
    "        spatial_shapes = []\n",
    "        for i, feat in enumerate(proj_feats):\n",
    "            _, _, h, w = feat.shape\n",
    "            # [b, c, h, w] -> [b, h*w, c]\n",
    "            feat_flatten.append(feat.flatten(2).permute(0, 2, 1))\n",
    "            # [num_levels, 2]\n",
    "            spatial_shapes.append([h, w])\n",
    "\n",
    "        # [b, l, c]\n",
    "        feat_flatten = torch.concat(feat_flatten, 1)\n",
    "        return feat_flatten, spatial_shapes\n",
    "\n",
    "feat_flatten, spatial_shapes = _get_encoder_input(encoder_outs)\n",
    "print(feat_flatten.shape)\n",
    "print(spatial_shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21457c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8400, 4])\n",
      "torch.Size([1, 8400, 1])\n"
     ]
    }
   ],
   "source": [
    "eval_spatial_size = [640, 640]\n",
    "feat_strides = [8, 16, 32]\n",
    "eps = 1e-2\n",
    "\n",
    "def _generate_anchors(spatial_shapes = None,\n",
    "                      grid_size = 0.05,\n",
    "                      dtype=torch.float32,\n",
    "                      device = 'cpu'):\n",
    "    \n",
    "    if spatial_shapes is None:\n",
    "            spatial_shapes = []\n",
    "            eval_h, eval_w = eval_spatial_size\n",
    "            for s in feat_strides:\n",
    "                spatial_shapes.append([int(eval_h / s), int(eval_w / s)])\n",
    "\n",
    "    anchors = []\n",
    "    for lvl, (h, w) in enumerate(spatial_shapes):\n",
    "        grid_y, grid_x = torch.meshgrid(torch.arange(h), torch.arange(w), indexing='ij')\n",
    "        grid_xy = torch.stack([grid_x, grid_y], dim=-1)\n",
    "        grid_xy = (grid_xy.unsqueeze(0) + 0.5) / torch.tensor([w, h], dtype=dtype)\n",
    "        wh = torch.ones_like(grid_xy) * grid_size * (2.0 ** lvl)\n",
    "        lvl_anchors = torch.concat([grid_xy, wh], dim=-1).reshape(-1, h * w, 4)\n",
    "        anchors.append(lvl_anchors)\n",
    "\n",
    "    anchors = torch.concat(anchors, dim=1).to(device)\n",
    "    valid_mask = ((anchors > eps) * (anchors < 1 - eps)).all(-1, keepdim=True)\n",
    "    anchors = torch.log(anchors / (1 - anchors))\n",
    "    anchors = torch.where(valid_mask, anchors, torch.inf)\n",
    "\n",
    "    return anchors, valid_mask\n",
    "\n",
    "anchors, valid_mask = _generate_anchors(spatial_shapes)\n",
    "print(anchors.shape)\n",
    "print(valid_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48f0dda8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 300, 256])\n",
      "torch.Size([2, 300, 80])\n",
      "torch.Size([2, 300, 4])\n"
     ]
    }
   ],
   "source": [
    "def _select_topk(memory: torch.Tensor, outputs_logits: torch.Tensor, outputs_anchors_unact: torch.Tensor, topk: int):\n",
    "        \n",
    "        _, topk_ind = torch.topk(outputs_logits.max(-1).values, topk, dim=-1)\n",
    "\n",
    "        topk_ind: torch.Tensor\n",
    "\n",
    "        topk_anchors = outputs_anchors_unact.gather(dim=1, \\\n",
    "            index=topk_ind.unsqueeze(-1).repeat(1, 1, outputs_anchors_unact.shape[-1]))\n",
    "\n",
    "        topk_logits = outputs_logits.gather(dim=1, \\\n",
    "            index=topk_ind.unsqueeze(-1).repeat(1, 1, outputs_logits.shape[-1]))\n",
    "\n",
    "        topk_memory = memory.gather(dim=1, \\\n",
    "            index=topk_ind.unsqueeze(-1).repeat(1, 1, memory.shape[-1]))\n",
    "\n",
    "        return topk_memory, topk_logits, topk_anchors\n",
    "    \n",
    "\n",
    "rand_output_memory = torch.rand(size=(2, 8400, 256))\n",
    "rand_enc_outputs_logits = torch.rand(size=(2, 8400, 80))\n",
    "rand_anchors = torch.rand(size=(2, 8400, 4))\n",
    "\n",
    "rand_topk_memory, rand_topk_logits, rand_topk_anchors = _select_topk(rand_output_memory, rand_enc_outputs_logits, rand_anchors, 300)\n",
    "print(rand_topk_memory.shape)\n",
    "print(rand_topk_logits.shape)\n",
    "print(rand_topk_anchors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6c98e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers, act='relu'):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        h = [hidden_dim] * (num_layers - 1)\n",
    "        self.layers = nn.ModuleList(nn.Linear(n, k) for n, k in zip([input_dim] + h, h + [output_dim]))\n",
    "        self.act = get_activation(act)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = self.act(layer(x)) if i < self.num_layers - 1 else layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96909919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 300, 256])\n",
      "torch.Size([2, 300, 4])\n"
     ]
    }
   ],
   "source": [
    "enc_output = nn.Sequential(OrderedDict([\n",
    "            ('proj', nn.Linear(256, 256)),\n",
    "            ('norm', nn.LayerNorm(256,)),\n",
    "        ]))\n",
    "\n",
    "enc_score_head = nn.Linear(256, 80)\n",
    "\n",
    "enc_bbox_head = MLP(256, 256, 4, 3, act='relu')\n",
    "\n",
    "def _get_decoder_input(memory: torch.Tensor,\n",
    "                       spatial_shapes,\n",
    "                       denoising_logits = None,\n",
    "                       denoising_bbox_unact = None\n",
    "                       ):\n",
    "    \n",
    "    # prepare input for decoder\n",
    "    anchors, valid_mask = _generate_anchors(spatial_shapes, device=memory.device)\n",
    "    if memory.shape[0] > 1:\n",
    "        anchors = anchors.repeat(memory.shape[0], 1, 1)\n",
    "    \n",
    "    # memory = torch.where(valid_mask, memory, 0)\n",
    "    # TODO fix type error for onnx export\n",
    "    memory = valid_mask.to(memory.dtype) * memory\n",
    "    # print(memory.shape)\n",
    "    \n",
    "    output_memory = enc_output(memory)\n",
    "    enc_outputs_logits = enc_score_head(output_memory)\n",
    "    # print(output_memory.shape, enc_outputs_logits.shape)\n",
    "    \n",
    "    enc_topk_bboxes_list, enc_topk_logits_list = [], []\n",
    "    enc_topk_memory, enc_topk_logits, enc_topk_anchors = _select_topk(output_memory, enc_outputs_logits, anchors, topk=300)\n",
    "    enc_topk_bbox_unact = enc_bbox_head(enc_topk_memory) + enc_topk_anchors\n",
    "    # print(enc_topk_memory.shape, enc_topk_logits.shape, enc_topk_anchors.shape)\n",
    "    \n",
    "    enc_topk_bboxes = F.sigmoid(enc_topk_bbox_unact)\n",
    "    enc_topk_bboxes_list.append(enc_topk_bboxes)\n",
    "    enc_topk_logits_list.append(enc_topk_logits)\n",
    "    \n",
    "    #\n",
    "    content = enc_topk_memory.detach()\n",
    "    enc_topk_bbox_unact = enc_topk_bbox_unact.detach()\n",
    "    \n",
    "    if denoising_bbox_unact is not None:\n",
    "        enc_topk_bbox_unact = torch.concat([denoising_bbox_unact, enc_topk_bbox_unact], dim=1)\n",
    "        content = torch.concat([denoising_logits, content], dim=1)\n",
    "    \n",
    "    return content, enc_topk_bbox_unact, enc_topk_bboxes_list, enc_topk_logits_list\n",
    "\n",
    "content, enc_topk_bbox_unact, enc_topk_bboxes_list, enc_topk_logits_list = _get_decoder_input(feat_flatten, spatial_shapes)\n",
    "\n",
    "print(content.shape)\n",
    "print(enc_topk_bbox_unact.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e04bb9",
   "metadata": {},
   "source": [
    "## denoising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "24339ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def box_cxcywh_to_xyxy(x):\n",
    "    x_c, y_c, w, h = x.unbind(-1)\n",
    "    b = [(x_c - 0.5 * w.clamp(min=0.0)), (y_c - 0.5 * h.clamp(min=0.0)),\n",
    "         (x_c + 0.5 * w.clamp(min=0.0)), (y_c + 0.5 * h.clamp(min=0.0))]\n",
    "    return torch.stack(b, dim=-1)\n",
    "\n",
    "\n",
    "def box_xyxy_to_cxcywh(x):\n",
    "    x0, y0, x1, y1 = x.unbind(-1)\n",
    "    b = [(x0 + x1) / 2, (y0 + y1) / 2,\n",
    "         (x1 - x0), (y1 - y0)]\n",
    "    return torch.stack(b, dim=-1)\n",
    "\n",
    "def inverse_sigmoid(x: torch.Tensor, eps: float=1e-5) -> torch.Tensor:\n",
    "    x = x.clip(min=0., max=1.)\n",
    "    return torch.log(x.clip(min=eps) / (1 - x).clip(min=eps))\n",
    "\n",
    "def get_contrastive_denoising_training_group(targets,\n",
    "                                             num_classes,\n",
    "                                             num_queries,\n",
    "                                             class_embed,\n",
    "                                             num_denoising=100,\n",
    "                                             label_noise_ratio=0.5,\n",
    "                                             box_noise_scale=1.0,):\n",
    "    \"\"\"cnd\"\"\"\n",
    "    if num_denoising <= 0:\n",
    "        return None, None, None, None\n",
    "\n",
    "    num_gts = [len(t['labels']) for t in targets]\n",
    "    device = targets[0]['labels'].device\n",
    "\n",
    "    max_gt_num = max(num_gts)\n",
    "    if max_gt_num == 0:\n",
    "        return None, None, None, None\n",
    "\n",
    "    num_group = num_denoising // max_gt_num\n",
    "    num_group = 1 if num_group == 0 else num_group\n",
    "    # pad gt to max_num of a batch\n",
    "    bs = len(num_gts)\n",
    "\n",
    "    input_query_class = torch.full([bs, max_gt_num], num_classes, dtype=torch.int32, device=device)\n",
    "    input_query_bbox = torch.zeros([bs, max_gt_num, 4], device=device)\n",
    "    pad_gt_mask = torch.zeros([bs, max_gt_num], dtype=torch.bool, device=device)\n",
    "\n",
    "    for i in range(bs):\n",
    "        num_gt = num_gts[i]\n",
    "        if num_gt > 0:\n",
    "            input_query_class[i, :num_gt] = targets[i]['labels']\n",
    "            input_query_bbox[i, :num_gt] = targets[i]['boxes']\n",
    "            pad_gt_mask[i, :num_gt] = 1\n",
    "    # each group has positive and negative queries.\n",
    "    input_query_class = input_query_class.tile([1, 2 * num_group])\n",
    "    input_query_bbox = input_query_bbox.tile([1, 2 * num_group, 1])\n",
    "    pad_gt_mask = pad_gt_mask.tile([1, 2 * num_group])\n",
    "    # positive and negative mask\n",
    "    negative_gt_mask = torch.zeros([bs, max_gt_num * 2, 1], device=device)\n",
    "    negative_gt_mask[:, max_gt_num:] = 1\n",
    "    negative_gt_mask = negative_gt_mask.tile([1, num_group, 1])\n",
    "    positive_gt_mask = 1 - negative_gt_mask\n",
    "    # contrastive denoising training positive index\n",
    "    positive_gt_mask = positive_gt_mask.squeeze(-1) * pad_gt_mask\n",
    "    dn_positive_idx = torch.nonzero(positive_gt_mask)[:, 1]\n",
    "    dn_positive_idx = torch.split(dn_positive_idx, [n * num_group for n in num_gts])\n",
    "    # total denoising queries\n",
    "    num_denoising = int(max_gt_num * 2 * num_group)\n",
    "\n",
    "    if label_noise_ratio > 0:\n",
    "        mask = torch.rand_like(input_query_class, dtype=torch.float) < (label_noise_ratio * 0.5)\n",
    "        # randomly put a new one here\n",
    "        new_label = torch.randint_like(mask, 0, num_classes, dtype=input_query_class.dtype)\n",
    "        input_query_class = torch.where(mask & pad_gt_mask, new_label, input_query_class)\n",
    "\n",
    "    if box_noise_scale > 0:\n",
    "        known_bbox = box_cxcywh_to_xyxy(input_query_bbox)\n",
    "        diff = torch.tile(input_query_bbox[..., 2:] * 0.5, [1, 1, 2]) * box_noise_scale\n",
    "        rand_sign = torch.randint_like(input_query_bbox, 0, 2) * 2.0 - 1.0\n",
    "        rand_part = torch.rand_like(input_query_bbox)\n",
    "        rand_part = (rand_part + 1.0) * negative_gt_mask + rand_part * (1 - negative_gt_mask)\n",
    "        known_bbox += (rand_sign * rand_part * diff)\n",
    "        known_bbox = torch.clip(known_bbox, min=0.0, max=1.0)\n",
    "        input_query_bbox = box_xyxy_to_cxcywh(known_bbox)\n",
    "        # FIXME, RT-DETR do not have this \n",
    "        input_query_bbox[input_query_bbox < 0] *= -1\n",
    "        input_query_bbox_unact = inverse_sigmoid(input_query_bbox)\n",
    "\n",
    "    input_query_logits = class_embed(input_query_class)\n",
    "\n",
    "    tgt_size = num_denoising + num_queries\n",
    "    attn_mask = torch.full([tgt_size, tgt_size], False, dtype=torch.bool, device=device)\n",
    "    # match query cannot see the reconstruction\n",
    "    attn_mask[num_denoising:, :num_denoising] = True\n",
    "\n",
    "    # reconstruct cannot see each other\n",
    "    for i in range(num_group):\n",
    "        if i == 0:\n",
    "            attn_mask[max_gt_num * 2 * i: max_gt_num * 2 * (i + 1), max_gt_num * 2 * (i + 1): num_denoising] = True\n",
    "        if i == num_group - 1:\n",
    "            attn_mask[max_gt_num * 2 * i: max_gt_num * 2 * (i + 1), :max_gt_num * i * 2] = True\n",
    "        else:\n",
    "            attn_mask[max_gt_num * 2 * i: max_gt_num * 2 * (i + 1), max_gt_num * 2 * (i + 1): num_denoising] = True\n",
    "            attn_mask[max_gt_num * 2 * i: max_gt_num * 2 * (i + 1), :max_gt_num * 2 * i] = True\n",
    "\n",
    "    dn_meta = {\n",
    "        \"dn_positive_idx\": dn_positive_idx,\n",
    "        \"dn_num_group\": num_group,\n",
    "        \"dn_num_split\": [num_denoising, num_queries]\n",
    "    }\n",
    "\n",
    "    # print(input_query_class.shape) # torch.Size([4, 196, 256])\n",
    "    # print(input_query_bbox.shape) # torch.Size([4, 196, 4])\n",
    "    # print(attn_mask.shape) # torch.Size([496, 496])\n",
    "\n",
    "    return input_query_logits, input_query_bbox_unact, attn_mask, dn_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aa6a6b08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 192, 256])\n"
     ]
    }
   ],
   "source": [
    "sim_target = [\n",
    "    {\n",
    "        'labels': torch.tensor([ 0,  0, 44, 45, 40, 60]),\n",
    "        'boxes': torch.tensor([\n",
    "            [0.4113, 0.7784, 0.3679, 0.4433],\n",
    "            [0.6646, 0.9303, 0.0822, 0.1371],\n",
    "            [0.1509, 0.7139, 0.0535, 0.2653],\n",
    "            [0.1712, 0.6474, 0.0269, 0.0747],\n",
    "            [0.3275, 0.3854, 0.4594, 0.7607],\n",
    "            [0.7270, 0.4966, 0.5400, 0.9843]\n",
    "        ])\n",
    "    },\n",
    "    {\n",
    "        'labels': torch.tensor([58, 61]),\n",
    "        'boxes': torch.tensor([\n",
    "            [0.4982, 0.3783, 0.0425, 0.0107],\n",
    "            [0.6783, 0.4064, 0.0088, 0.0426],\n",
    "        ])\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "def get_contrastive_denoising_training_group(targets,\n",
    "                                             num_classes,\n",
    "                                             num_queries,\n",
    "                                             class_embed,\n",
    "                                             num_denoising=100,\n",
    "                                             label_noise_ratio=0.5,\n",
    "                                             box_noise_scale=1.0,):\n",
    "    \"\"\"cnd\"\"\"\n",
    "    if num_denoising <= 0:\n",
    "        return None, None, None, None\n",
    "\n",
    "    num_gts = [len(t['labels']) for t in targets]\n",
    "    device = targets[0]['labels'].device\n",
    "\n",
    "    max_gt_num = max(num_gts)\n",
    "    if max_gt_num == 0:\n",
    "        return None, None, None, None\n",
    "\n",
    "    num_group = num_denoising // max_gt_num\n",
    "    num_group = 1 if num_group == 0 else num_group\n",
    "    # pad gt to max_num of a batch\n",
    "    bs = len(num_gts)\n",
    "\n",
    "    input_query_class = torch.full([bs, max_gt_num], num_classes, dtype=torch.int32, device=device)\n",
    "    input_query_bbox = torch.zeros([bs, max_gt_num, 4], device=device)\n",
    "    pad_gt_mask = torch.zeros([bs, max_gt_num], dtype=torch.bool, device=device)\n",
    "\n",
    "    for i in range(bs):\n",
    "        num_gt = num_gts[i]\n",
    "        if num_gt > 0:\n",
    "            input_query_class[i, :num_gt] = targets[i]['labels']\n",
    "            input_query_bbox[i, :num_gt] = targets[i]['boxes']\n",
    "            pad_gt_mask[i, :num_gt] = 1\n",
    "    # each group has positive and negative queries.\n",
    "    input_query_class = input_query_class.tile([1, 2 * num_group])\n",
    "    input_query_bbox = input_query_bbox.tile([1, 2 * num_group, 1])\n",
    "    pad_gt_mask = pad_gt_mask.tile([1, 2 * num_group])\n",
    "    # positive and negative mask\n",
    "    negative_gt_mask = torch.zeros([bs, max_gt_num * 2, 1], device=device)\n",
    "    negative_gt_mask[:, max_gt_num:] = 1\n",
    "    negative_gt_mask = negative_gt_mask.tile([1, num_group, 1])\n",
    "    positive_gt_mask = 1 - negative_gt_mask\n",
    "    # contrastive denoising training positive index\n",
    "    positive_gt_mask = positive_gt_mask.squeeze(-1) * pad_gt_mask\n",
    "    dn_positive_idx = torch.nonzero(positive_gt_mask)[:, 1]\n",
    "    dn_positive_idx = torch.split(dn_positive_idx, [n * num_group for n in num_gts])\n",
    "    # total denoising queries\n",
    "    num_denoising = int(max_gt_num * 2 * num_group)\n",
    "\n",
    "    if label_noise_ratio > 0:\n",
    "        mask = torch.rand_like(input_query_class, dtype=torch.float) < (label_noise_ratio * 0.5)\n",
    "        # randomly put a new one here\n",
    "        new_label = torch.randint_like(mask, 0, num_classes, dtype=input_query_class.dtype)\n",
    "        input_query_class = torch.where(mask & pad_gt_mask, new_label, input_query_class)\n",
    "\n",
    "    if box_noise_scale > 0:\n",
    "        known_bbox = box_cxcywh_to_xyxy(input_query_bbox)\n",
    "        diff = torch.tile(input_query_bbox[..., 2:] * 0.5, [1, 1, 2]) * box_noise_scale\n",
    "        rand_sign = torch.randint_like(input_query_bbox, 0, 2) * 2.0 - 1.0\n",
    "        rand_part = torch.rand_like(input_query_bbox)\n",
    "        rand_part = (rand_part + 1.0) * negative_gt_mask + rand_part * (1 - negative_gt_mask)\n",
    "        known_bbox += (rand_sign * rand_part * diff)\n",
    "        known_bbox = torch.clip(known_bbox, min=0.0, max=1.0)\n",
    "        input_query_bbox = box_xyxy_to_cxcywh(known_bbox)\n",
    "        # FIXME, RT-DETR do not have this \n",
    "        input_query_bbox[input_query_bbox < 0] *= -1\n",
    "        input_query_bbox_unact = inverse_sigmoid(input_query_bbox)\n",
    "\n",
    "    input_query_logits = class_embed(input_query_class)\n",
    "\n",
    "    tgt_size = num_denoising + num_queries\n",
    "    attn_mask = torch.full([tgt_size, tgt_size], False, dtype=torch.bool, device=device)\n",
    "    # match query cannot see the reconstruction\n",
    "    attn_mask[num_denoising:, :num_denoising] = True\n",
    "\n",
    "    # reconstruct cannot see each other\n",
    "    for i in range(num_group):\n",
    "        if i == 0:\n",
    "            attn_mask[max_gt_num * 2 * i: max_gt_num * 2 * (i + 1), max_gt_num * 2 * (i + 1): num_denoising] = True\n",
    "        if i == num_group - 1:\n",
    "            attn_mask[max_gt_num * 2 * i: max_gt_num * 2 * (i + 1), :max_gt_num * i * 2] = True\n",
    "        else:\n",
    "            attn_mask[max_gt_num * 2 * i: max_gt_num * 2 * (i + 1), max_gt_num * 2 * (i + 1): num_denoising] = True\n",
    "            attn_mask[max_gt_num * 2 * i: max_gt_num * 2 * (i + 1), :max_gt_num * 2 * i] = True\n",
    "\n",
    "    dn_meta = {\n",
    "        \"dn_positive_idx\": dn_positive_idx,\n",
    "        \"dn_num_group\": num_group,\n",
    "        \"dn_num_split\": [num_denoising, num_queries]\n",
    "    }\n",
    "\n",
    "    # print(input_query_class.shape) # torch.Size([4, 196, 256])\n",
    "    # print(input_query_bbox.shape) # torch.Size([4, 196, 4])\n",
    "    # print(attn_mask.shape) # torch.Size([496, 496])\n",
    "\n",
    "    return input_query_logits, input_query_bbox_unact, attn_mask, dn_meta\n",
    "\n",
    "\n",
    "denoising_class_embed = nn.Embedding(80+1, 256, padding_idx=80)\n",
    "input_query_logits, input_query_bbox_unact, attn_mask, dn_meta = get_contrastive_denoising_training_group(sim_target, 80, 300,\n",
    "                                         denoising_class_embed, 100, 0.5, 1.0)\n",
    "\n",
    "# 100 // max_gt_num(6) = 16  num_denoising = 16 * mat_gt_num * 2 = 32 * 6 = 192\n",
    "print(input_query_logits.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2c9b7cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 492, 256])\n",
      "torch.Size([2, 492, 4])\n"
     ]
    }
   ],
   "source": [
    "content, enc_topk_bbox_unact, enc_topk_bboxes_list, enc_topk_logits_list = \\\n",
    "        _get_decoder_input(feat_flatten, spatial_shapes, input_query_logits, input_query_bbox_unact)\n",
    "        \n",
    "print(content.shape)\n",
    "print(enc_topk_bbox_unact.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b92eb99",
   "metadata": {},
   "source": [
    "## DecoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3c5fa0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bias_init_with_prob(prior_prob=0.01):\n",
    "    \"\"\"initialize conv/fc bias value according to a given probability value.\"\"\"\n",
    "    bias_init = float(-math.log((1 - prior_prob) / prior_prob))\n",
    "    return bias_init\n",
    "\n",
    "class Gate(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super(Gate, self).__init__()\n",
    "        self.gate = nn.Linear(2 * d_model, 2 * d_model)\n",
    "        bias = bias_init_with_prob(0.5)\n",
    "        init.constant_(self.gate.bias, bias)\n",
    "        init.constant_(self.gate.weight, 0)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        gate_input = torch.cat([x1, x2], dim=-1)\n",
    "        gates = torch.sigmoid(self.gate(gate_input))\n",
    "        gate1, gate2 = gates.chunk(2, dim=-1)\n",
    "        return self.norm(gate1 * x1 + gate2 * x2)\n",
    "\n",
    "\n",
    "class TransformerDecoderLayer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 d_model = 256,\n",
    "                 n_head = 8,\n",
    "                 dim_feedforward = 1024,\n",
    "                 dropout = 0.,\n",
    "                 activation = 'relu',\n",
    "                 n_levels = 3,\n",
    "                 n_points = 4,\n",
    "                 cross_attn_method = 'default',\n",
    "                 layer_scale = None\n",
    "                 ):\n",
    "        super(TransformerDecoderLayer, self).__init__()\n",
    "        if layer_scale is not None:\n",
    "            dim_feedforward = round(layer_scale * dim_feedforward)\n",
    "            d_model = round(layer_scale * d_model)\n",
    "        \n",
    "        # self attention\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, n_head, dropout=dropout, batch_first=True)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # cross attention\n",
    "        self.cross_attn = MSDeformableAttention(d_model, n_head, n_levels, n_points,\n",
    "                                                method=cross_attn_method)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        \n",
    "        # gate\n",
    "        self.gateway = Gate(d_model)\n",
    "        \n",
    "        # ffn\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.activation = get_activation(activation)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "        self.dropout4 = nn.Dropout(dropout)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        self._reset_parameters()\n",
    "    \n",
    "    def _reset_parameters(self):\n",
    "        init.xavier_uniform_(self.linear1.weight)\n",
    "        init.xavier_uniform_(self.linear2.weight)\n",
    "\n",
    "    def with_pos_embed(self, tensor, pos):\n",
    "        return tensor if pos is None else tensor + pos\n",
    "\n",
    "    def forward_ffn(self, tgt):\n",
    "        return self.linear2(self.dropout3(self.activation(self.linear1(tgt))))\n",
    "    \n",
    "    def forward(self,\n",
    "                target,\n",
    "                reference_points,\n",
    "                value,\n",
    "                spatial_shapes,\n",
    "                attn_mask = None,\n",
    "                query_pos_embed = None\n",
    "                ):\n",
    "\n",
    "        # self attention\n",
    "        q = k = self.with_pos_embed(target, query_pos_embed)\n",
    "        \n",
    "        target2, _ = self.self_attn(q, k, value=target, attn_mask = attn_mask)\n",
    "        target = target + self.dropout1(target2)\n",
    "        \n",
    "        # cross attention\n",
    "        target2 = self.cross_attn(\n",
    "            self.with_pos_embed(target, query_pos_embed),\n",
    "            reference_points,\n",
    "            value,\n",
    "            spatial_shapes\n",
    "        )\n",
    "        \n",
    "        target = self.gateway(target, self.dropout2(target2))\n",
    "        \n",
    "        # ffn\n",
    "        target2 = self.forward_ffn(target2)\n",
    "        target = target + self.dropout4(target2)\n",
    "        target = self.norm3(target.clamp(min=-65504, max=65504))\n",
    "        \n",
    "        return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5226b4c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 8, 32, 6400])\n",
      "torch.Size([2, 8, 32, 1600])\n",
      "torch.Size([2, 8, 32, 400])\n",
      "torch.Size([2, 492, 256])\n"
     ]
    }
   ],
   "source": [
    "def value_op(memory, value_proj, value_scale, memory_mask, memory_spatial_shapes):\n",
    "        \"\"\"\n",
    "        Preprocess values for MSDeformableAttention.\n",
    "        \"\"\"\n",
    "        value = value_proj(memory) if value_proj is not None else memory\n",
    "        value = F.interpolate(memory, size=value_scale) if value_scale is not None else value\n",
    "        if memory_mask is not None:\n",
    "            value = value * memory_mask.to(value.dtype).unsqueeze(-1)\n",
    "        value = value.reshape(value.shape[0], value.shape[1], 8, -1)\n",
    "        split_shape = [h * w for h, w in memory_spatial_shapes]\n",
    "        return value.permute(0, 2, 3, 1).split(split_shape, dim=-1)\n",
    "\n",
    "value = value_op(feat_flatten, None, None, None, spatial_shapes)\n",
    "for i in value:\n",
    "    print(i.shape)\n",
    "\n",
    "decoder_layer_instance = TransformerDecoderLayer(layer_scale=1)\n",
    "\n",
    "ref_points_detach = F.sigmoid(enc_topk_bbox_unact)\n",
    "ref_points_input = ref_points_detach.unsqueeze(2)       # [2, 492, 1, 4]\n",
    "\n",
    "output = content\n",
    "\n",
    "query_pos_head = MLP(4, 2 * 256, 256, 2, act='relu')\n",
    "query_pos_embed = query_pos_head(ref_points_detach).clamp(min=-10, max=10)\n",
    "\n",
    "decoder_output = decoder_layer_instance.forward(\n",
    "    target=output, reference_points=ref_points_input, value=value, spatial_shapes=spatial_shapes, attn_mask=attn_mask, query_pos_embed=query_pos_embed\n",
    ")\n",
    "\n",
    "print(decoder_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d735ed91",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a5228ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LQE(nn.Module):\n",
    "    def __init__(self, k, hidden_dim, num_layers, reg_max, act='relu'):\n",
    "        super(LQE, self).__init__()\n",
    "        self.k = k\n",
    "        self.reg_max = reg_max\n",
    "        self.reg_conf = MLP(4 * (k + 1), hidden_dim, 1, num_layers, act=act)\n",
    "        init.constant_(self.reg_conf.layers[-1].bias, 0)\n",
    "        init.constant_(self.reg_conf.layers[-1].weight, 0)\n",
    "    \n",
    "    def forward(self, scores, pred_corners):\n",
    "        B, L, _ = pred_corners.size()\n",
    "        prob = F.softmax(pred_corners.reshape(B, L, 4, self.reg_max + 1), dim=-1)\n",
    "        prob_topk, _ = prob.topk(self.k, dim=-1)\n",
    "        stat = torch.cat([prob_topk, prob_topk.mean(dim=-1, keepdim=True)], dim=-1)\n",
    "        quality_score = self.reg_conf(stat.reshape(B, L, -1))\n",
    "        return scores + quality_score\n",
    "\n",
    "\n",
    "def weighting_function(reg_max, up, reg_scale, deploy=False):\n",
    "    \"\"\"\n",
    "    Generates the non-uniform Weighting Function W(n) for bounding box regression.\n",
    "\n",
    "    Args:\n",
    "        reg_max (int): Max number of the discrete bins.\n",
    "        up (Tensor): Controls upper bounds of the sequence,\n",
    "                     where maximum offset is up * H / W.\n",
    "        reg_scale (float): Controls the curvature of the Weighting Function.\n",
    "                           Larger values result in flatter weights near the central axis W(reg_max/2)=0\n",
    "                           and steeper weights at both ends.\n",
    "        deploy (bool): If True, uses deployment mode settings.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: Sequence of Weighting Function.\n",
    "    \"\"\"\n",
    "    if deploy:\n",
    "        upper_bound1 = (abs(up[0]) * abs(reg_scale)).item()\n",
    "        upper_bound2 = (abs(up[0]) * abs(reg_scale) * 2).item()\n",
    "        step = (upper_bound1 + 1) ** (2 / (reg_max - 2))\n",
    "        left_values = [-(step) ** i + 1 for i in range(reg_max // 2 - 1, 0, -1)]\n",
    "        right_values = [(step) ** i - 1 for i in range(1, reg_max // 2)]\n",
    "        values = [-upper_bound2] + left_values + [torch.zeros_like(up[0][None])] + right_values + [upper_bound2]\n",
    "        return torch.tensor(values, dtype=up.dtype, device=up.device)\n",
    "    else:\n",
    "        upper_bound1 = abs(up[0]) * abs(reg_scale)\n",
    "        upper_bound2 = abs(up[0]) * abs(reg_scale) * 2\n",
    "        step = (upper_bound1 + 1) ** (2 / (reg_max - 2))\n",
    "        left_values = [-(step) ** i + 1 for i in range(reg_max // 2 - 1, 0, -1)]\n",
    "        right_values = [(step) ** i - 1 for i in range(1, reg_max // 2)]\n",
    "        values = [-upper_bound2] + left_values + [torch.zeros_like(up[0][None])] + right_values + [upper_bound2]\n",
    "        return torch.cat(values, 0)\n",
    "\n",
    "def distance2bbox(points, distance, reg_scale):\n",
    "    \"\"\"\n",
    "    Decodes edge-distances into bounding box coordinates.\n",
    "\n",
    "    Args:\n",
    "        points (Tensor): (B, N, 4) or (N, 4) format, representing [x, y, w, h],\n",
    "                         where (x, y) is the center and (w, h) are width and height.\n",
    "        distance (Tensor): (B, N, 4) or (N, 4), representing distances from the\n",
    "                           point to the left, top, right, and bottom boundaries.\n",
    "\n",
    "        reg_scale (float): Controls the curvature of the Weighting Function.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: Bounding boxes in (N, 4) or (B, N, 4) format [cx, cy, w, h].\n",
    "    \"\"\"\n",
    "    reg_scale = abs(reg_scale)\n",
    "    x1 = points[..., 0] - (0.5 * reg_scale + distance[..., 0]) * (points[..., 2] / reg_scale)\n",
    "    y1 = points[..., 1] - (0.5 * reg_scale + distance[..., 1]) * (points[..., 3] / reg_scale)\n",
    "    x2 = points[..., 0] + (0.5 * reg_scale + distance[..., 2]) * (points[..., 2] / reg_scale)\n",
    "    y2 = points[..., 1] + (0.5 * reg_scale + distance[..., 3]) * (points[..., 3] / reg_scale)\n",
    "\n",
    "    bboxes = torch.stack([x1, y1, x2, y2], -1)\n",
    "\n",
    "    return box_xyxy_to_cxcywh(bboxes)\n",
    "\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer Decoder implementing Fine-grained Distribution Refinement (FDR).\n",
    "\n",
    "    This decoder refines object detection predictions through iterative updates across multiple layers,\n",
    "    utilizing attention mechanisms, location quality estimators, and distribution refinement techniques\n",
    "    to improve bounding box accuracy and robustness.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 hidden_dim,\n",
    "                 decoder_layer,\n",
    "                 decoder_layer_wide,\n",
    "                 num_layers,\n",
    "                 num_head,\n",
    "                 reg_max,\n",
    "                 reg_scale,\n",
    "                 up,\n",
    "                 eval_idx = -1,\n",
    "                 layer_scale = 2,\n",
    "                 act = 'relu'\n",
    "                 ):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.layer_scale = layer_scale\n",
    "        self.num_head = num_head\n",
    "        self.eval_idx = eval_idx if eval_idx >= 0 else num_layers + eval_idx\n",
    "        \n",
    "        self.up, self.reg_scale, self.reg_max = up, reg_scale, reg_max\n",
    "        self.layers = nn.ModuleList(\n",
    "            [copy.deepcopy(decoder_layer) for _ in range(self.eval_idx + 1)] + \n",
    "            [copy.deepcopy(decoder_layer_wide) for _ in range(num_layers - self.eval_idx - 1)]\n",
    "        )\n",
    "\n",
    "        self.lqe_layers = nn.ModuleList([copy.deepcopy(LQE(4, 64, 2, reg_max, act=act)) for _ in range(num_layers)])\n",
    "        \n",
    "    def value_op(self, memory, value_proj, value_scale, memory_mask, memory_spatial_shapes):\n",
    "        \"\"\"\n",
    "        Preprocess values for MSDeformableAttention.\n",
    "        \"\"\"\n",
    "        value = value_proj(memory) if value_proj is not None else memory\n",
    "        value = F.interpolate(memory, size=value_scale) if value_scale is not None else value\n",
    "        if memory_mask is not None:\n",
    "            value = value * memory_mask.to(value.dtype).unsqueeze(-1)\n",
    "        value = value.reshape(value.shape[0], value.shape[1], self.num_head, -1)\n",
    "        split_shape = [h * w for h, w in memory_spatial_shapes]\n",
    "        return value.permute(0, 2, 3, 1).split(split_shape, dim=-1)\n",
    "\n",
    "    def convert_to_deploy(self):\n",
    "        self.project = weighting_function(self.reg_max, self.up, self.reg_scale, deploy=True)\n",
    "        self.layers = self.layers[:self.eval_idx + 1]\n",
    "        self.lqe_layers = nn.ModuleList([nn.Identity()] * (self.eval_idx) + [self.lqe_layers[self.eval_idx]])\n",
    "    \n",
    "    def forward(self,\n",
    "                target,\n",
    "                ref_points_unact,\n",
    "                memory,\n",
    "                spatial_shapes,\n",
    "                bbox_head,\n",
    "                score_head,\n",
    "                query_pos_head,\n",
    "                pre_bbox_head,\n",
    "                integral,\n",
    "                up,\n",
    "                reg_scale,\n",
    "                attn_mask=None,\n",
    "                memory_mask=None,\n",
    "                dn_meta=None):\n",
    "\n",
    "        output = target\n",
    "        output_detach = pred_corners_undetach = 0\n",
    "        value = self.value_op(memory, None, None, memory_mask, spatial_shapes)\n",
    "        \n",
    "        dec_out_bboxes = []\n",
    "        dec_out_logits = []\n",
    "        dec_out_pred_corners = [] \n",
    "        dec_out_refs = []\n",
    "        \n",
    "        if not hasattr(self, 'project'):\n",
    "            project = weighting_function(self.reg_max, up, reg_scale)\n",
    "        else:\n",
    "            project = self.project\n",
    "        \n",
    "        ref_points_detach = F.sigmoid(ref_points_unact)\n",
    "        \n",
    "        for i, layer in enumerate(self.layers):\n",
    "            ref_points_input = ref_points_detach.unsqueeze(2)\n",
    "            query_pos_embed = query_pos_head(ref_points_detach).clamp(min=-10, max=10)\n",
    "            \n",
    "            # TODO Adjust scale if needed for detachable wider layers\n",
    "            if i >= self.eval_idx + 1 and self.layer_scale > 1:\n",
    "                query_pos_embed = F.interpolate(query_pos_embed, scale_factor=self.layer_scale)\n",
    "                value = self.value_op(memory, None, query_pos_embed.shape[-1], memory_mask, spatial_shapes)\n",
    "                output = F.interpolate(output, size=query_pos_embed.shape[-1])\n",
    "                output_detach = output.detach()\n",
    "\n",
    "            output = layer(output, ref_points_input, value, spatial_shapes, attn_mask, query_pos_embed)\n",
    "\n",
    "            if i == 0:\n",
    "                # Initial bounding box predicitions with inverse sigmoid refinement\n",
    "                pre_bboxes = F.sigmoid(pre_bbox_head(output) + inverse_sigmoid(ref_points_detach))\n",
    "                pre_scores = score_head[0](output)\n",
    "                ref_points_initial = pre_bboxes.detach()\n",
    "            \n",
    "            # Refine bounding box corners using FDR, integrating previous layer's corrections\n",
    "            pred_corners = bbox_head[i](output + output_detach) + pred_corners_undetach\n",
    "            inter_ref_bbox = distance2bbox(ref_points_initial, integral(pred_corners, project), reg_scale)\n",
    "            \n",
    "            if self.training or i == self.eval_idx:\n",
    "                scores = score_head[i](output)\n",
    "                # Lqe does not affect the performance here.\n",
    "                scores = self.lqe_layers[i](scores, pred_corners)\n",
    "                dec_out_logits.append(scores)\n",
    "                dec_out_bboxes.append(inter_ref_bbox)\n",
    "                dec_out_pred_corners.append(pred_corners)\n",
    "                dec_out_refs.append(ref_points_initial)\n",
    "\n",
    "                if not self.training:\n",
    "                    break\n",
    "\n",
    "            pred_corners_undetach = pred_corners\n",
    "            ref_points_detach = inter_ref_bbox.detach()\n",
    "            output_detach = output.detach()\n",
    "\n",
    "        return torch.stack(dec_out_bboxes), torch.stack(dec_out_logits), \\\n",
    "               torch.stack(dec_out_pred_corners), torch.stack(dec_out_refs), pre_bboxes, pre_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "71a2c2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Integral(nn.Module):\n",
    "    \"\"\"\n",
    "    A static layer that calculates integral results from a distribution.\n",
    "\n",
    "    This layer computes the target location using the formula: `sum{Pr(n) * W(n)}`,\n",
    "    where Pr(n) is the softmax probability vector representing the discrete\n",
    "    distribution, and W(n) is the non-uniform Weighting Function.\n",
    "\n",
    "    Args:\n",
    "        reg_max (int): Max number of the discrete bins. Default is 32.\n",
    "                       It can be adjusted based on the dataset or task requirements.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, reg_max=32):\n",
    "        super(Integral, self).__init__()\n",
    "        self.reg_max = reg_max\n",
    "\n",
    "    def forward(self, x, project):\n",
    "        shape = x.shape\n",
    "        x = F.softmax(x.reshape(-1, self.reg_max + 1), dim=1)\n",
    "        x = F.linear(x, project.to(x.device)).reshape(-1, 4)\n",
    "        return x.reshape(list(shape[:-1]) + [-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "22093584",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 80\n",
    "\n",
    "feat_channels = [256, 256, 256]\n",
    "feat_strides = [8, 16, 32]\n",
    "hidden_dim = 256\n",
    "num_levels = 3\n",
    "nhead=8\n",
    "num_layers = 6\n",
    "dim_feedforward = 1024\n",
    "dropout=0.\n",
    "activation=\"relu\"\n",
    "eval_idx = -1\n",
    "num_queries = 300\n",
    "num_denoising = 100\n",
    "label_noise_ratio = 0.5\n",
    "box_noise_scale = 1.0\n",
    "# NEW\n",
    "reg_max = 32\n",
    "reg_scale = 4\n",
    "\n",
    "mlp_act='relu'\n",
    "\n",
    "# Auxiliary decoder layers dimension scaling\n",
    "# \"eg. If num_layers: 6 eval_idx: -4,\n",
    "# then layer 3, 4, 5 are auxiliary decoder layers.\"\n",
    "layer_scale = 1  # 2\n",
    "\n",
    "num_points = [3, 6, 3] # [4, 4, 4] [3, 6, 3]\n",
    "cross_attn_method = 'default' # 'default', discrete\n",
    "query_select_method = 'default' # 'default', agnostic\n",
    "\n",
    "\n",
    "scaled_dim = round(layer_scale*hidden_dim)\n",
    "\n",
    "denoising_class_embed = nn.Embedding(80+1, 256, padding_idx=80)\n",
    "\n",
    "dec_bbox_head = nn.ModuleList(\n",
    "            [MLP(hidden_dim, hidden_dim, 4 * (reg_max+1), 3, act=mlp_act) for _ in range(eval_idx + 1)]\n",
    "          + [MLP(scaled_dim, scaled_dim, 4 * (reg_max+1), 3, act=mlp_act) for _ in range(num_layers - eval_idx - 1)]\n",
    "          )\n",
    "\n",
    "dec_score_head = nn.ModuleList(\n",
    "            [nn.Linear(hidden_dim, num_classes) for _ in range(eval_idx + 1)]\n",
    "          + [nn.Linear(scaled_dim, num_classes) for _ in range(num_layers - eval_idx - 1)])\n",
    "\n",
    "pre_bbox_head = MLP(hidden_dim, hidden_dim, 4, 3, act=mlp_act)\n",
    "\n",
    "integral = Integral(reg_max)\n",
    "up = nn.Parameter(torch.tensor([0.5]), requires_grad=False)\n",
    "reg_scale = nn.Parameter(torch.tensor([reg_scale]), requires_grad=False)\n",
    "\n",
    "\n",
    "decoder_layer = TransformerDecoderLayer(hidden_dim, nhead, dim_feedforward, dropout, \\\n",
    "            activation, num_levels, num_points, cross_attn_method=cross_attn_method)\n",
    "decoder_layer_wide = TransformerDecoderLayer(hidden_dim, nhead, dim_feedforward, dropout, \\\n",
    "            activation, num_levels, num_points, cross_attn_method=cross_attn_method, layer_scale=layer_scale)\n",
    "decoder = TransformerDecoder(hidden_dim, decoder_layer, decoder_layer_wide, num_layers, nhead,\n",
    "                                          reg_max, reg_scale, up, eval_idx, layer_scale, act=activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9fda0530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 2, 492, 4])\n",
      "torch.Size([6, 2, 492, 80])\n",
      "torch.Size([6, 2, 492, 132])\n",
      "torch.Size([6, 2, 492, 4])\n",
      "torch.Size([2, 492, 4])\n",
      "torch.Size([2, 492, 80])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "memory, spatial_shapes = _get_encoder_input(encoder_outs)\n",
    "\n",
    "if num_denoising > 0:\n",
    "    denoising_logits, denoising_bbox_unact, attn_mask, dn_meta = \\\n",
    "                get_contrastive_denoising_training_group(sim_target, \\\n",
    "                    num_classes,\n",
    "                    num_queries,\n",
    "                    denoising_class_embed,\n",
    "                    num_denoising=num_denoising,\n",
    "                    label_noise_ratio=label_noise_ratio,\n",
    "                    box_noise_scale=1.0,\n",
    "                    )\n",
    "else:\n",
    "    denoising_logits, denoising_bbox_unact, attn_mask, dn_meta = None, None, None, None\n",
    "\n",
    "init_ref_contents, init_ref_points_unact, enc_topk_bboxes_list, enc_topk_logits_list = \\\n",
    "            _get_decoder_input(memory, spatial_shapes, \n",
    "                               denoising_logits, denoising_bbox_unact)\n",
    "\n",
    "\n",
    "\n",
    "# decoder\n",
    "out_bboxes, out_logits, out_corners, out_refs, pre_bboxes, pre_logits = decoder.forward(\n",
    "            init_ref_contents,\n",
    "            init_ref_points_unact,\n",
    "            memory,\n",
    "            spatial_shapes,\n",
    "            dec_bbox_head,\n",
    "            dec_score_head,\n",
    "            query_pos_head,\n",
    "            pre_bbox_head,\n",
    "            integral,\n",
    "            up,\n",
    "            reg_scale,\n",
    "            attn_mask=attn_mask,\n",
    "            dn_meta=dn_meta)\n",
    "\n",
    "print(out_bboxes.shape)\n",
    "print(out_logits.shape)\n",
    "print(out_corners.shape)\n",
    "print(out_refs.shape)\n",
    "print(pre_bboxes.shape)\n",
    "print(pre_logits.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
